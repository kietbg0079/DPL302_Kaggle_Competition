{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzaSbFKexa1S",
        "outputId": "1e7cf316-46fa-49aa-a115-b618ca188905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.9 MB 7.8 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbvrTJdaxxPo",
        "outputId": "81a0c5b5-5875-4460-8d9f-7448dfb444da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 237 kB 69.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 71.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 48.3 MB 1.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 54.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 92 kB 11.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 53.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.8 MB/s \n",
            "\u001b[?25h  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tf-models-official==2.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "r6Tm5X-epG-h",
        "outputId": "134df321-a152-4e15-e7dd-104ea8963ec7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b4bb93a8-bae6-45c3-8516-edaebc2ebba1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b4bb93a8-bae6-45c3-8516-edaebc2ebba1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "User uploaded file \"kaggle.json\" with length 66 bytes\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "# # Then move kaggle.json into the folder where the API expects to find it.\n",
        "# !mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9r8NF0Kw72-",
        "outputId": "e885b65c-3d44-4855-aefb-45ff83f67bbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading spr22-AIG201m-DPL301m.zip to /content\n",
            " 94% 171M/182M [00:02<00:00, 112MB/s] \n",
            "100% 182M/182M [00:03<00:00, 63.4MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !kaggle competitions download -c spr22-AIG201m-DPL301m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2tKahLoMgD_",
        "outputId": "01f915ff-cf83-44d6-8ff2-78ca7d0b7a84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_r95Cusy5HV"
      },
      "source": [
        "#Data preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BL7Tv0yEZgak"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import zipfile as zp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDdV7PKyF-cN"
      },
      "outputs": [],
      "source": [
        "#path = \"/content/spr22-AIG201m-DPL301m.zip\"\n",
        "path = '/content/drive/MyDrive/work/dpl/workspace.zip'\n",
        "#path_img = '/content/drive/MyDrive/dataset/image.zip'\n",
        "json_path = \"/content/dpl/devset_images_metadata.json\"\n",
        "work_dir = \"/content/dpl\"\n",
        "\n",
        "img_dir = \"/content/dpl/training\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uI6BwcZjuSQd"
      },
      "outputs": [],
      "source": [
        "#Extract workdir\n",
        "\n",
        "zipfile = zp.ZipFile('/content/drive/MyDrive/work/dpl/workspace.zip')\n",
        "zipfile.extractall(work_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5qqSdbgFWx5"
      },
      "outputs": [],
      "source": [
        "#Extract image\n",
        "\n",
        "# imgzip = zp.ZipFile(path_img)\n",
        "# imgzip.extractall(work_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UB1ylJKizxGQ"
      },
      "outputs": [],
      "source": [
        "#Json metadata\n",
        "# with open(json_path) as f:\n",
        "#   data = json.load(f)\n",
        "\n",
        "# df = pd.DataFrame.from_dict(data[\"images\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_YUuDi4GP79"
      },
      "outputs": [],
      "source": [
        "# for i in os.listdir('/content/dpl/testset_images/testset_images'):\n",
        "#   os.remove('/content/dpl/testset_images/testset_images/' + i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6URRaBUPpbQ"
      },
      "outputs": [],
      "source": [
        "#Load csv file\n",
        "\n",
        "# train_raw = pd.read_csv(img_dir + \"/train.csv\")\n",
        "# test_raw = pd.read_csv(img_dir + \"/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6B65rucCxctQ"
      },
      "outputs": [],
      "source": [
        "# train_text_raw = train_raw['info']\n",
        "# test_text_raw = test_raw['info']\n",
        "\n",
        "# train_label_raw = train_raw['label']\n",
        "# test_label_raw = test_raw['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7kXWrW4Vfo0"
      },
      "outputs": [],
      "source": [
        "# train_set = df.loc[:, ['image_id', 'title', 'description', 'user_tags']]\n",
        "\n",
        "# label = pd.read_csv('/content/dpl/devset_images_gt.csv')\n",
        "# label = label.astype({\"id\":str})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnmWtopRVuc5"
      },
      "outputs": [],
      "source": [
        "# train_set['user_tags'] = train_set.apply(lambda x: ' '.join(x['user_tags']), axis=1)\n",
        "# train_set['info'] = train_set['title'].astype(str) + \" \" + train_set['description'].astype(str) + \" \" + train_set['user_tags']\n",
        "\n",
        "# combine = train_set.join(label.set_index('id'), on='image_id')\n",
        "# combine = combine.loc[:, ['image_id', 'info', 'label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQCugLgsDZI3"
      },
      "outputs": [],
      "source": [
        "#Filter error image\n",
        "# error_id = []\n",
        "# with open('/content/dpl/error_id.txt', 'r') as f:\n",
        "#   for line in f:\n",
        "#     error_id.append(line.rstrip())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s8MqWvEEKFO"
      },
      "outputs": [],
      "source": [
        "#combine = combine_raw[~combine_raw['image_id'].isin(error_id)].reset_index().drop(['index'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDObOEWKnBTj"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "\n",
        "# train_lstdir_t = os.listdir('/content/dpl/data/test/1')\n",
        "# train_lstdir_n = os.listdir('/content/dpl/data/test/0')\n",
        "\n",
        "# for i in train_lstdir_t:\n",
        "#   shutil.move(os.path.join(img_dir, 'test/1/' + i), os.path.join(img_dir, 'train/1'))\n",
        "\n",
        "# for i in train_lstdir_n:\n",
        "#   shutil.move(os.path.join(img_dir, 'test/0/' + i), os.path.join(img_dir, 'train/0'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gZ2161EIFsFm"
      },
      "outputs": [],
      "source": [
        "#Check image dataset length == dataframe row \n",
        "\n",
        "#print(f\"Train: {len(train_raw.index) == len(os.listdir(os.path.join(img_dir, 'train/0'))) + len(os.listdir(os.path.join(img_dir, 'train/1')))} \")\n",
        "#print(f\"Test: {len(test_raw.index) == len(os.listdir(os.path.join(img_dir, 'test/0'))) + len(os.listdir(os.path.join(img_dir, 'test/1')))} \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yl4OsesPnwzL"
      },
      "source": [
        "\n",
        "\n",
        "# Crawl data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4lq2mNpWcB7"
      },
      "outputs": [],
      "source": [
        "# img_url_lst = combine['image_url'].to_list()\n",
        "# img_id_lst = combine['image_id'].to_list()\n",
        "# img_label_lst = combine['label'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwUgX0roulgq"
      },
      "outputs": [],
      "source": [
        "# len(img_url_lst)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpNk71XIi0BE"
      },
      "outputs": [],
      "source": [
        "# from bs4 import BeautifulSoup\n",
        "# import requests\n",
        "# import urllib\n",
        "# import re\n",
        "# import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp7LZiGxkJW0"
      },
      "outputs": [],
      "source": [
        "# !mkdir /content/dpl/data\n",
        "# !mkdir /content/dpl/data/train\n",
        "# !mkdir /content/dpl/data/train/1\n",
        "# !mkdir /content/dpl/data/train/0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OM3XpPcMNJs6"
      },
      "outputs": [],
      "source": [
        "# def crawL_img(url, id, label):\n",
        "#   fp = urllib.request.urlopen(url)\n",
        "#   mybytes = fp.read()\n",
        "\n",
        "#   mystr = mybytes.decode(\"utf8\")\n",
        "#   fp.close()\n",
        "\n",
        "#   soup = BeautifulSoup(mystr)\n",
        "#   for link in soup.findAll('div', attrs={'class': 'attribution-info'}):\n",
        "#     att_info = link.a['href']\n",
        "\n",
        "#   link_down = urllib.request.urlopen('http://www.flickr.com' + att_info + id + '/sizes/o/')\n",
        "#   newbytes = link_down.read()\n",
        "\n",
        "#   htmlstr = newbytes.decode(\"utf8\")\n",
        "#   link_down.close()\n",
        "\n",
        "#   resoup = BeautifulSoup(htmlstr)\n",
        "\n",
        "#   for link in resoup.findAll('a'):\n",
        "#     try:\n",
        "#       if link.get('href').startswith('https://live.staticflickr.com'):\n",
        "#         link_down = link.get('href')\n",
        "#     except:\n",
        "#       pass\n",
        "\n",
        "#   img_data = requests.get(link_down).content\n",
        "#   if label==0:\n",
        "#     path_train = \"/content/dpl/data/train/0\"\n",
        "#   else:\n",
        "#     path_train = \"/content/dpl/data/train/1\"\n",
        "    \n",
        "#   with open(os.path.join(path_train, id+'.jpg'), 'wb') as handler:\n",
        "#     handler.write(img_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3AoldKcmQa-"
      },
      "outputs": [],
      "source": [
        "# error_id = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXVEEh6al1p8"
      },
      "outputs": [],
      "source": [
        "# for i in range(175,len(img_url_lst)):\n",
        "#   try:\n",
        "#     crawL_img(img_url_lst[i], img_id_lst[i], img_label_lst[i])\n",
        "#   except:\n",
        "#     error_id.append(img_id_lst[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtIadiCgfN-R"
      },
      "outputs": [],
      "source": [
        "# shutil.move('/content/image.zip.zip', '/content/drive/MyDrive/dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKUHaqD1fHHO"
      },
      "outputs": [],
      "source": [
        "# with open('/content/your_file.txt', 'w') as f:\n",
        "#     for item in error_id:\n",
        "#         f.write(\"%s\\n\" % item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA9uz0HfcSrQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8b9af936-3ab0-4446-f9e5-f40e1327b96e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/work/dpl/workspace.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# import shutil\n",
        "# shutil.make_archive('/content/drive/MyDrive/work/dpl/workspace', 'zip', '/content/dpl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59RfGsTqb8n5"
      },
      "outputs": [],
      "source": [
        "#len(os.listdir('/content/dpl/data/train/1'))\n",
        "#len(os.listdir('/content/dpl/data/train/0')) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWoYMfidKLwv"
      },
      "source": [
        "# Split data to test and train set (Kaggle origin data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYIHyhFDLGB9"
      },
      "outputs": [],
      "source": [
        "# !mkdir /content/dpl/training/test\n",
        "# !mkdir /content/dpl/training/test/0\n",
        "# !mkdir /content/dpl/training/test/1\n",
        "\n",
        "# !mkdir /content/dpl/training/val\n",
        "# !mkdir /content/dpl/training/val/0\n",
        "# !mkdir /content/dpl/training/val/1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMdWfoIfMITv"
      },
      "outputs": [],
      "source": [
        "#train_df = train_df[~train_df['image_id'].isin(lst_id_val)]\n",
        "# train_token = train_df['info'].to_list()\n",
        "# label_train = train_df['label'].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejp3agHjppJb"
      },
      "outputs": [],
      "source": [
        "# train_df.drop(['image_url'], axis=1).to_csv('/content/dpl/data/train_data.csv')\n",
        "# test_df.drop(['image_url'], axis=1).to_csv('/content/dpl/data/test_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09Pg15SbKEOD",
        "outputId": "81594a0d-91cf-4f61-e09a-f4178c24e08c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4704"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#len(os.listdir(os.path.join(img_dir, 'train/0'))) + len(os.listdir(os.path.join(img_dir, 'train/1'))) + len(os.listdir(os.path.join(img_dir, 'test/0'))) + len(os.listdir(os.path.join(img_dir, 'test/1')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPkJEj6uDyzg"
      },
      "outputs": [],
      "source": [
        "# from struct import unpack\n",
        "# from tqdm import tqdm\n",
        "# import os\n",
        "\n",
        "\n",
        "# marker_mapping = {\n",
        "#     0xffd8: \"Start of Image\",\n",
        "#     0xffe0: \"Application Default Header\",\n",
        "#     0xffdb: \"Quantization Table\",\n",
        "#     0xffc0: \"Start of Frame\",\n",
        "#     0xffc4: \"Define Huffman Table\",\n",
        "#     0xffda: \"Start of Scan\",\n",
        "#     0xffd9: \"End of Image\"\n",
        "# }\n",
        "\n",
        "\n",
        "# class JPEG:\n",
        "#     def __init__(self, image_file):\n",
        "#         with open(image_file, 'rb') as f:\n",
        "#             self.img_data = f.read()\n",
        "    \n",
        "#     def decode(self):\n",
        "#         data = self.img_data\n",
        "#         while(True):\n",
        "#             marker, = unpack(\">H\", data[0:2])\n",
        "#             # print(marker_mapping.get(marker))\n",
        "#             if marker == 0xffd8:\n",
        "#                 data = data[2:]\n",
        "#             elif marker == 0xffd9:\n",
        "#                 return\n",
        "#             elif marker == 0xffda:\n",
        "#                 data = data[-2:]\n",
        "#             else:\n",
        "#                 lenchunk, = unpack(\">H\", data[2:4])\n",
        "#                 data = data[2+lenchunk:]            \n",
        "#             if len(data)==0:\n",
        "#                 break        \n",
        "\n",
        "\n",
        "# bads_0 = []\n",
        "# test_lstdir = os.listdir('/content/dpl/testset_images/testset_images')\n",
        "# for img in test_lstdir:\n",
        "#   image = os.path.join('/content/dpl/testset_images/testset_images',img)\n",
        "#   image = JPEG(image) \n",
        "#   try:\n",
        "#     image.decode()   \n",
        "#   except:\n",
        "#     bads_0.append(img)\n",
        "\n",
        "\n",
        "# for name in bads_0:\n",
        "#   os.remove(os.path.join('/content/dpl/data/test/0',name))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0wuA5WE0PK5",
        "outputId": "d9e2056d-b7b8-4571-b452-46dd7232ff1c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2602"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(test_lstdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE73SU0REfBf"
      },
      "outputs": [],
      "source": [
        "# train_0 = os.listdir('/content/dpl/data/test/0')\n",
        "# train_1 = os.listdir('/content/dpl/data/test/1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFvY4MHMtNUi"
      },
      "source": [
        "# Mixed Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DH9P_7Xx8SR",
        "outputId": "6650568f-718a-44dd-86f5-bb6cf34ffbf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: opencv-python-headless==4.1.2.30 in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless==4.1.2.30) (1.21.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install opencv-python-headless==4.1.2.30"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZO2swMMgtMn6"
      },
      "outputs": [],
      "source": [
        "# !mkdir /content/dpltg/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMvtwZpwvcfa"
      },
      "outputs": [],
      "source": [
        "# for i in os.listdir('/content/dpltg/training/val/1'):\n",
        "#   shutil.copy('/content/dpltg/training/val/1/' + i, '/content/dpltg/data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0WnswPQvu68"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os \n",
        "\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upRjptOoyEyy"
      },
      "outputs": [],
      "source": [
        "lst_img = df['image_id'].to_list()\n",
        "lst_text = df['info'].to_list()\n",
        "lst_label = df['label'].to_list()\n",
        "\n",
        "img_arr = []\n",
        "text_arr = []\n",
        "label_arr = []\n",
        "\n",
        "error_id = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Augmentation**"
      ],
      "metadata": {
        "id": "j0B_JJtlsCDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install textattack"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9-wzISV6eEA",
        "outputId": "27b627ca-b122-4eeb-bf90-ef91ce5ea607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting textattack\n",
            "  Downloading textattack-0.3.5-py3-none-any.whl (415 kB)\n",
            "\u001b[K     |████████████████████████████████| 415 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.7.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from textattack) (3.7.1)\n",
            "Collecting terminaltables\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Collecting bert-score>=0.3.5\n",
            "  Downloading bert_score-0.3.11-py3-none-any.whl (60 kB)\n",
            "\u001b[K     |████████████████████████████████| 60 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting pinyin==0.4.0\n",
            "  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 53.7 MB/s \n",
            "\u001b[?25hCollecting pycld2\n",
            "  Downloading pycld2-0.41.tar.gz (41.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.4 MB 1.1 MB/s \n",
            "\u001b[?25hCollecting word2number\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from textattack) (0.5.3)\n",
            "Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from textattack) (7.1.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from textattack) (8.13.0)\n",
            "Collecting flair\n",
            "  Downloading flair-0.11.3-py3-none-any.whl (401 kB)\n",
            "\u001b[K     |████████████████████████████████| 401 kB 42.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from textattack) (4.64.0)\n",
            "Collecting num2words\n",
            "  Downloading num2words-0.5.10-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 10.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.2 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.21.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from textattack) (3.7)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.7/dist-packages (from textattack) (0.42.1)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.11.0+cu113)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.4.1)\n",
            "Collecting datasets==1.15\n",
            "  Downloading datasets-1.15.0-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 32.6 MB/s \n",
            "\u001b[?25hCollecting OpenHowNet\n",
            "  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
            "Collecting transformers>=3.3.0\n",
            "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 45.0 MB/s \n",
            "\u001b[?25hCollecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Collecting lru-dict\n",
            "  Downloading lru-dict-1.1.7.tar.gz (10 kB)\n",
            "Collecting lemminflect\n",
            "  Downloading lemminflect-0.2.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 31.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from textattack) (1.3.5)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 52.9 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.5.0-py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==1.15->textattack) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==1.15->textattack) (4.11.4)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.15->textattack) (0.3.5.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets==1.15->textattack) (0.70.13)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.15->textattack) (6.0.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==1.15->textattack) (2.23.0)\n",
            "Collecting huggingface-hub<0.1.0,>=0.0.19\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 78.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert-score>=0.3.5->textattack) (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets==1.15->textattack) (4.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets==1.15->textattack) (3.13)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==1.15->textattack) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->textattack) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->textattack) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.15->textattack) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.15->textattack) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.15->textattack) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==1.15->textattack) (1.24.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.3.0->textattack) (2022.6.2)\n",
            "Collecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.8 MB/s \n",
            "\u001b[?25hCollecting transformers>=3.3.0\n",
            "  Downloading transformers-4.19.3-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 41.5 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 32.4 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 33.7 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.19.0-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 55.7 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 32.2 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 54.8 MB/s \n",
            "\u001b[?25hCollecting transformers>=3.3.0\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 47.9 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 47.2 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.16.1-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 46.3 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.16.0-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 49.3 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 51.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 61.3 MB/s \n",
            "\u001b[?25hCollecting transformers>=3.3.0\n",
            "  Downloading transformers-4.14.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 47.3 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 52.5 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 51.9 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.12.4-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 69.3 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 70.3 MB/s \n",
            "\u001b[?25h  Downloading transformers-4.12.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 57.6 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.7 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 50.2 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 71.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.15->textattack) (2.0.12)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==1.15->textattack) (21.4.0)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting conllu>=4.0\n",
            "  Downloading conllu-4.4.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair->textattack) (4.2.6)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 50.6 MB/s \n",
            "\u001b[?25hCollecting janome\n",
            "  Downloading Janome-0.4.2-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 460 kB/s \n",
            "\u001b[?25hCollecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 56.9 MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair->textattack) (1.0.2)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-2.0.0.tar.gz (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting pptree\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "Requirement already satisfied: gdown==4.4.0 in /usr/local/lib/python3.7/dist-packages (from flair->textattack) (4.4.0)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair->textattack) (3.6.0)\n",
            "Collecting hyperopt>=0.2.7\n",
            "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair->textattack) (0.8.9)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 24.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown==4.4.0->flair->textattack) (4.6.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair->textattack) (1.14.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair->textattack) (6.0.0)\n",
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 66.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair->textattack) (2.6.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair->textattack) (0.16.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.2.7->flair->textattack) (1.3.0)\n",
            "Collecting requests>=2.19.0\n",
            "  Downloading requests-2.28.0-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting importlib-metadata\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==1.15->textattack) (3.8.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.11.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair->textattack) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair->textattack) (1.1.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->flair->textattack) (0.2.5)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from num2words->textattack) (0.6.2)\n",
            "Collecting anytree\n",
            "  Downloading anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 606 kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from OpenHowNet->textattack) (57.4.0)\n",
            "Building wheels for collected packages: pinyin, mpld3, overrides, sqlitedict, langdetect, lru-dict, pptree, pycld2, sacremoses, wikipedia-api, word2number\n",
            "  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630494 sha256=0bc3c0857145c7988f312e86fcab737a79a4502f4aee46a7fba526da813ba722\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/7d/a6/f2fe72950e89e84982e1982f7a68805a37773693188a9c889c\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=4c6b141e13b7dec91e699575e199c6fef9b71190e2bf5330a960a17ebccd797b\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10187 sha256=503850f0e72c2877ea791190753f3e1fbabc7d4e7bf58021f1eaf43a38525777\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.0.0-py3-none-any.whl size=15736 sha256=645334614131bfc72cb4b164fe97b618c4b04f48dfcb2236bc7274dab0799ae8\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/dd/2e/0ed4a25cb73fc30c7ea8d10b50acb7226175736067e40a7ea3\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=8e446c93ad86ce4b7100bd8ce557684da6885ffeb6326b86c5b1819c4e02d5fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for lru-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lru-dict: filename=lru_dict-1.1.7-cp37-cp37m-linux_x86_64.whl size=28433 sha256=d5e37053ad652a220a77133c4796280ff4ddc9ba5e772a3a6680fcd885789fc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/9d/0b/4e/aa8fec9833090cd52bcd76f92f9d95e1ee7b915c12093663b4\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=5899fda309149443b56b4f9244ab5b35a88336dbb7a76b8cd97b92341245ffdd\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/e8/7d/a9c3c19b4722608a0d8b05a38c36bc3f230c43becd2a46794b\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp37-cp37m-linux_x86_64.whl size=9834272 sha256=42c4e253b94c87c7379cf202d9683bfffe321b369f2e49c9398220ef72646d06\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/e4/58/ed2e9f43c07d617cc81fe7aff0fc6e42b16c9cf6afe960b614\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=7446a3b0cfb288e79cbef44a05013a660a4eead61c7c398c1b34cb6184ba9baf\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13477 sha256=f7f1613e462210d38d14cb58b9059f367df6471e43a3d868e5c6baabf23dd7ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5582 sha256=5fe9be5162f01f9d1de9f840f8eb91c5e3b58337473355277b8fe523f325befa\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "Successfully built pinyin mpld3 overrides sqlitedict langdetect lru-dict pptree pycld2 sacremoses wikipedia-api word2number\n",
            "Installing collected packages: multidict, frozenlist, yarl, requests, pyyaml, importlib-metadata, asynctest, async-timeout, aiosignal, tokenizers, sentencepiece, sacremoses, py4j, overrides, huggingface-hub, fsspec, aiohttp, xxhash, wikipedia-api, transformers, sqlitedict, segtok, pptree, mpld3, langdetect, konoha, janome, hyperopt, ftfy, deprecated, conllu, bpemb, anytree, word2number, terminaltables, pycld2, pinyin, OpenHowNet, num2words, lru-dict, lemminflect, language-tool-python, flair, datasets, bert-score, textattack\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.11.4\n",
            "    Uninstalling importlib-metadata-4.11.4:\n",
            "      Successfully uninstalled importlib-metadata-4.11.4\n",
            "  Attempting uninstall: hyperopt\n",
            "    Found existing installation: hyperopt 0.1.2\n",
            "    Uninstalling hyperopt-0.1.2:\n",
            "      Successfully uninstalled hyperopt-0.1.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.7 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.28.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed OpenHowNet-2.0 aiohttp-3.8.1 aiosignal-1.2.0 anytree-2.8.0 async-timeout-4.0.2 asynctest-0.13.0 bert-score-0.3.11 bpemb-0.3.3 conllu-4.4.2 datasets-1.15.0 deprecated-1.2.13 flair-0.11.3 frozenlist-1.3.0 fsspec-2022.5.0 ftfy-6.1.1 huggingface-hub-0.0.19 hyperopt-0.2.7 importlib-metadata-3.10.1 janome-0.4.2 konoha-4.6.5 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.2 lru-dict-1.1.7 mpld3-0.3 multidict-6.0.2 num2words-0.5.10 overrides-3.1.0 pinyin-0.4.0 pptree-3.1 py4j-0.10.9.5 pycld2-0.41 pyyaml-6.0 requests-2.28.0 sacremoses-0.0.53 segtok-1.5.11 sentencepiece-0.1.95 sqlitedict-2.0.0 terminaltables-3.1.10 textattack-0.3.5 tokenizers-0.10.3 transformers-4.12.2 wikipedia-api-0.5.4 word2number-1.1 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "f1HlNPXs64dI",
        "outputId": "433f516b-9d63-492f-b53f-d6547e7218ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'New... How much on a flood plain architecture holiday inn express skyline snowdome tamworth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# from textattack.augmentation import WordNetAugmenter, EmbeddingAugmenter, EasyDataAugmenter\n",
        "# wordnet_aug = WordNetAugmenter()\n",
        "# embed_aug = EmbeddingAugmenter()\n",
        "# easy_aug = EasyDataAugmenter()\n",
        "# easy_aug.augment(lst_text[1])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlfPdYYxySFg",
        "outputId": "ef66378b-9a5c-4aeb-8c1e-f9404d179027"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done 3\n",
            "Done 6\n",
            "Done 9\n",
            "Done 10\n",
            "Done 12\n",
            "Done 13\n",
            "Done 14\n",
            "Done 15\n",
            "Done 16\n",
            "Done 17\n",
            "Done 27\n",
            "Done 31\n",
            "Done 39\n",
            "Done 40\n",
            "Done 41\n",
            "Done 49\n",
            "Done 55\n",
            "Done 56\n",
            "Done 57\n",
            "Done 58\n",
            "Done 60\n",
            "Done 61\n",
            "Done 62\n",
            "Done 63\n",
            "Done 64\n",
            "Done 66\n",
            "Done 70\n",
            "Done 73\n",
            "Done 74\n",
            "Done 75\n",
            "Done 78\n",
            "Done 79\n",
            "Done 83\n",
            "Done 84\n",
            "Done 85\n",
            "Done 86\n",
            "Done 90\n",
            "Done 93\n",
            "Done 94\n",
            "Done 104\n",
            "Done 107\n",
            "Done 116\n",
            "Done 121\n",
            "Done 124\n",
            "Done 125\n",
            "Done 126\n",
            "Done 127\n",
            "Done 129\n",
            "Done 131\n",
            "Done 133\n",
            "Done 137\n",
            "Done 145\n",
            "Done 154\n",
            "Done 155\n",
            "Done 159\n",
            "Done 163\n",
            "Done 165\n",
            "Done 167\n",
            "Done 169\n",
            "Done 171\n",
            "Done 175\n",
            "Done 179\n",
            "Done 180\n",
            "Done 185\n",
            "Done 188\n",
            "Done 189\n",
            "Done 194\n",
            "Done 195\n",
            "Done 197\n",
            "Done 198\n",
            "Done 200\n",
            "Done 201\n",
            "Done 204\n",
            "Done 205\n",
            "Done 206\n",
            "Done 208\n",
            "Done 210\n",
            "Done 213\n",
            "Done 214\n",
            "Done 221\n",
            "Done 222\n",
            "Done 223\n",
            "Done 224\n",
            "Done 229\n",
            "Done 236\n",
            "Done 237\n",
            "Done 238\n",
            "Done 240\n",
            "Done 241\n",
            "Done 243\n",
            "Done 244\n",
            "Done 247\n",
            "Done 248\n",
            "Done 252\n",
            "Done 256\n",
            "Done 258\n",
            "Done 259\n",
            "Done 262\n",
            "Done 266\n",
            "Done 269\n",
            "Done 272\n",
            "Done 274\n",
            "Done 276\n",
            "Done 277\n",
            "Done 278\n",
            "Done 280\n",
            "Done 287\n",
            "Done 288\n",
            "Done 289\n",
            "Done 296\n",
            "Done 297\n",
            "Done 299\n",
            "Done 300\n",
            "Done 301\n",
            "Done 303\n",
            "Done 305\n",
            "Done 306\n",
            "Done 313\n",
            "Done 316\n",
            "Done 317\n",
            "Done 319\n",
            "Done 323\n",
            "Done 324\n",
            "Done 328\n",
            "Done 330\n",
            "Done 332\n",
            "Done 333\n",
            "Done 335\n",
            "Done 336\n",
            "Done 338\n",
            "Done 348\n",
            "Done 349\n",
            "Done 352\n",
            "Done 354\n",
            "Done 358\n",
            "Done 359\n",
            "Done 360\n",
            "Done 361\n",
            "Done 362\n",
            "Done 367\n",
            "Done 368\n",
            "Done 371\n",
            "Done 372\n",
            "Done 376\n",
            "Done 378\n",
            "Done 380\n",
            "Done 381\n",
            "Done 384\n",
            "Done 385\n",
            "Done 387\n",
            "Done 388\n",
            "Done 389\n",
            "Done 390\n",
            "Done 392\n",
            "Done 393\n",
            "Done 402\n",
            "Done 406\n",
            "Done 407\n",
            "Done 408\n",
            "Done 410\n",
            "Done 412\n",
            "Done 416\n",
            "Done 417\n",
            "Done 419\n",
            "Done 429\n",
            "Done 430\n",
            "Done 433\n",
            "Done 435\n",
            "Done 438\n",
            "Done 440\n",
            "Done 448\n",
            "Done 450\n",
            "Done 454\n",
            "Done 455\n",
            "Done 460\n",
            "Done 464\n",
            "Done 472\n",
            "Done 473\n",
            "Done 480\n",
            "Done 481\n",
            "Done 485\n",
            "Done 486\n",
            "Done 488\n",
            "Done 490\n",
            "Done 495\n",
            "Done 496\n",
            "Done 497\n",
            "Done 501\n",
            "Done 503\n",
            "Done 506\n",
            "Done 516\n",
            "Done 518\n",
            "Done 520\n",
            "Done 521\n",
            "Done 522\n",
            "Done 524\n",
            "Done 526\n",
            "Done 527\n",
            "Done 536\n",
            "Done 537\n",
            "Done 546\n",
            "Done 547\n",
            "Done 552\n",
            "Done 553\n",
            "Done 554\n",
            "Done 556\n",
            "Done 558\n",
            "Done 560\n",
            "Done 572\n",
            "Done 574\n",
            "Done 575\n",
            "Done 576\n",
            "Done 577\n",
            "Done 578\n",
            "Done 579\n",
            "Done 581\n",
            "Done 588\n",
            "Done 593\n",
            "Done 601\n",
            "Done 618\n",
            "Done 620\n",
            "Done 621\n",
            "Done 632\n",
            "Done 633\n",
            "Done 634\n",
            "Done 637\n",
            "Done 638\n",
            "Done 641\n",
            "Done 647\n",
            "Done 649\n",
            "Done 650\n",
            "Done 651\n",
            "Done 658\n",
            "Done 662\n",
            "Done 667\n",
            "Done 669\n",
            "Done 672\n",
            "Done 673\n",
            "Done 677\n",
            "Done 679\n",
            "Done 685\n",
            "Done 686\n",
            "Done 688\n",
            "Done 690\n",
            "Done 693\n",
            "Done 695\n",
            "Done 697\n",
            "Done 698\n",
            "Done 699\n",
            "Done 702\n",
            "Done 703\n",
            "Done 704\n",
            "Done 706\n",
            "Done 708\n",
            "Done 710\n",
            "Done 715\n",
            "Done 720\n",
            "Done 722\n",
            "Done 726\n",
            "Done 729\n",
            "Done 730\n",
            "Done 731\n",
            "Done 733\n",
            "Done 735\n",
            "Done 737\n",
            "Done 738\n",
            "Done 739\n",
            "Done 743\n",
            "Done 744\n",
            "Done 747\n",
            "Done 754\n",
            "Done 756\n",
            "Done 767\n",
            "Done 768\n",
            "Done 771\n",
            "Done 775\n",
            "Done 777\n",
            "Done 778\n",
            "Done 779\n",
            "Done 780\n",
            "Done 781\n",
            "Done 785\n",
            "Done 787\n",
            "Done 788\n",
            "Done 790\n",
            "Done 791\n",
            "Done 792\n",
            "Done 795\n",
            "Done 806\n",
            "Done 812\n",
            "Done 815\n",
            "Done 818\n",
            "Done 819\n",
            "Done 820\n",
            "Done 821\n",
            "Done 822\n",
            "Done 827\n",
            "Done 829\n",
            "Done 831\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model from cache /tmp/jieba.cache\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done 833\n",
            "Done 835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading model cost 0.783 seconds.\n",
            "Loading model cost 0.783 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done 839\n",
            "Done 840\n",
            "Done 841\n",
            "Done 842\n",
            "Done 846\n",
            "Done 848\n",
            "Done 849\n",
            "Done 855\n",
            "Done 859\n",
            "Done 865\n",
            "Done 867\n",
            "Done 869\n",
            "Done 870\n",
            "Done 872\n",
            "Done 875\n",
            "Done 880\n",
            "Done 881\n",
            "Done 884\n",
            "Done 886\n",
            "Done 890\n",
            "Done 893\n",
            "Done 894\n",
            "Done 895\n",
            "Done 897\n",
            "Done 902\n",
            "Done 908\n",
            "Done 911\n",
            "Done 918\n",
            "Done 919\n",
            "Done 928\n",
            "Done 929\n",
            "Done 930\n",
            "Done 935\n",
            "Done 939\n",
            "Done 940\n",
            "Done 941\n",
            "Done 945\n",
            "Done 946\n",
            "Done 949\n",
            "Done 955\n",
            "Done 956\n",
            "Done 957\n",
            "Done 958\n",
            "Done 963\n",
            "Done 964\n",
            "Done 965\n",
            "Done 966\n",
            "Done 967\n",
            "Done 968\n",
            "Done 969\n",
            "Done 971\n",
            "Done 972\n",
            "Done 975\n",
            "Done 978\n",
            "Done 979\n",
            "Done 982\n",
            "Done 984\n",
            "Done 987\n",
            "Done 993\n",
            "Done 994\n",
            "Done 995\n",
            "Done 998\n",
            "Done 1001\n",
            "Done 1002\n",
            "Done 1007\n",
            "Done 1009\n",
            "Done 1010\n",
            "Done 1021\n",
            "Done 1025\n",
            "Done 1028\n",
            "Done 1029\n",
            "Done 1032\n",
            "Done 1039\n",
            "Done 1041\n",
            "Done 1042\n",
            "Done 1047\n",
            "Done 1050\n",
            "Done 1052\n",
            "Done 1053\n",
            "Done 1054\n",
            "Done 1055\n",
            "Done 1059\n",
            "Done 1062\n",
            "Done 1064\n",
            "Done 1065\n",
            "Done 1066\n",
            "Done 1076\n",
            "Done 1078\n",
            "Done 1082\n",
            "Done 1083\n",
            "Done 1092\n",
            "Done 1101\n",
            "Done 1103\n",
            "Done 1110\n",
            "Done 1111\n",
            "Done 1112\n",
            "Done 1113\n",
            "Done 1114\n",
            "Done 1115\n",
            "Done 1116\n",
            "Done 1132\n",
            "Done 1136\n",
            "Done 1141\n",
            "Done 1149\n",
            "Done 1152\n",
            "Done 1154\n",
            "Done 1157\n",
            "Done 1160\n",
            "Done 1161\n",
            "Done 1166\n",
            "Done 1171\n",
            "Done 1172\n",
            "Done 1176\n",
            "Done 1178\n",
            "Done 1179\n",
            "Done 1180\n",
            "Done 1183\n",
            "Done 1187\n",
            "Done 1189\n",
            "Done 1191\n",
            "Done 1194\n",
            "Done 1195\n",
            "Done 1200\n",
            "Done 1201\n",
            "Done 1203\n",
            "Done 1204\n",
            "Done 1205\n",
            "Done 1206\n",
            "Done 1207\n",
            "Done 1214\n",
            "Done 1222\n",
            "Done 1229\n",
            "Done 1233\n",
            "Done 1238\n",
            "Done 1244\n",
            "Done 1253\n",
            "Done 1255\n",
            "Done 1256\n",
            "Done 1260\n",
            "Done 1263\n",
            "Done 1269\n",
            "Done 1270\n",
            "Done 1273\n",
            "Done 1275\n",
            "Done 1276\n",
            "Done 1280\n",
            "Done 1286\n",
            "Done 1290\n",
            "Done 1292\n",
            "Done 1293\n",
            "Done 1296\n",
            "Done 1298\n",
            "Done 1300\n",
            "Done 1305\n",
            "Done 1308\n",
            "Done 1312\n",
            "Done 1313\n",
            "Done 1314\n",
            "Done 1318\n",
            "Done 1323\n",
            "Done 1324\n",
            "Done 1325\n",
            "Done 1326\n",
            "Done 1331\n",
            "Done 1333\n",
            "Done 1334\n",
            "Done 1336\n",
            "Done 1337\n",
            "Done 1340\n",
            "Done 1346\n",
            "Done 1350\n",
            "Done 1352\n",
            "Done 1355\n",
            "Done 1357\n",
            "Done 1358\n",
            "Done 1359\n",
            "Done 1362\n",
            "Done 1363\n",
            "Done 1365\n",
            "Done 1366\n",
            "Done 1371\n",
            "Done 1374\n",
            "Done 1375\n",
            "Done 1379\n",
            "Done 1381\n",
            "Done 1384\n",
            "Done 1385\n",
            "Done 1387\n",
            "Done 1390\n",
            "Done 1395\n",
            "Done 1399\n",
            "Done 1402\n",
            "Done 1403\n",
            "Done 1408\n",
            "Done 1411\n",
            "Done 1412\n",
            "Done 1415\n",
            "Done 1416\n",
            "Done 1420\n",
            "Done 1425\n",
            "Done 1428\n",
            "Done 1430\n",
            "Done 1433\n",
            "Done 1438\n",
            "Done 1441\n",
            "Done 1442\n",
            "Done 1443\n",
            "Done 1449\n",
            "Done 1451\n",
            "Done 1454\n",
            "Done 1456\n",
            "Done 1457\n",
            "Done 1469\n",
            "Done 1470\n",
            "Done 1471\n",
            "Done 1475\n",
            "Done 1477\n",
            "Done 1479\n",
            "Done 1480\n",
            "Done 1481\n",
            "Done 1482\n",
            "Done 1485\n",
            "Done 1488\n",
            "Done 1491\n",
            "Done 1494\n",
            "Done 1496\n",
            "Done 1497\n",
            "Done 1498\n",
            "Done 1501\n",
            "Done 1503\n",
            "Done 1504\n",
            "Done 1505\n",
            "Done 1509\n",
            "Done 1512\n",
            "Done 1517\n",
            "Done 1524\n",
            "Done 1526\n",
            "Done 1527\n",
            "Done 1528\n",
            "Done 1530\n",
            "Done 1536\n",
            "Done 1537\n",
            "Done 1540\n",
            "Done 1544\n",
            "Done 1550\n",
            "Done 1551\n",
            "Done 1553\n",
            "Done 1556\n",
            "Done 1560\n",
            "Done 1561\n",
            "Done 1562\n",
            "Done 1566\n",
            "Done 1570\n",
            "Done 1572\n",
            "Done 1573\n",
            "Done 1575\n",
            "Done 1576\n",
            "Done 1577\n",
            "Done 1580\n",
            "Done 1581\n",
            "Done 1585\n",
            "Done 1587\n",
            "Done 1588\n",
            "Done 1590\n",
            "Done 1591\n",
            "Done 1595\n",
            "Done 1599\n",
            "Done 1600\n",
            "Done 1606\n",
            "Done 1608\n",
            "Done 1610\n",
            "Done 1612\n",
            "Done 1613\n",
            "Done 1616\n",
            "Done 1619\n",
            "Done 1621\n",
            "Done 1623\n",
            "Done 1627\n",
            "Done 1629\n",
            "Done 1634\n",
            "Done 1637\n",
            "Done 1638\n",
            "Done 1639\n",
            "Done 1641\n",
            "Done 1642\n",
            "Done 1643\n",
            "Done 1645\n",
            "Done 1652\n",
            "Done 1653\n",
            "Done 1655\n",
            "Done 1656\n",
            "Done 1661\n",
            "Done 1663\n",
            "Done 1666\n",
            "Done 1667\n",
            "Done 1676\n",
            "Done 1678\n",
            "Done 1679\n",
            "Done 1681\n",
            "Done 1682\n",
            "Done 1686\n",
            "Done 1687\n",
            "Done 1689\n",
            "Done 1690\n",
            "Done 1691\n",
            "Done 1692\n",
            "Done 1694\n",
            "Done 1695\n",
            "Done 1697\n",
            "Done 1698\n",
            "Done 1701\n",
            "Done 1702\n",
            "Done 1703\n",
            "Done 1706\n",
            "Done 1712\n",
            "Done 1713\n",
            "Done 1716\n",
            "Done 1722\n",
            "Done 1723\n",
            "Done 1724\n",
            "Done 1727\n",
            "Done 1730\n",
            "Done 1732\n",
            "Done 1733\n",
            "Done 1734\n",
            "Done 1736\n",
            "Done 1737\n",
            "Done 1738\n",
            "Done 1740\n",
            "Done 1741\n",
            "Done 1744\n",
            "Done 1745\n",
            "Done 1748\n",
            "Done 1750\n",
            "Done 1751\n",
            "Done 1752\n",
            "Done 1755\n",
            "Done 1759\n",
            "Done 1760\n",
            "Done 1766\n",
            "Done 1769\n",
            "Done 1772\n",
            "Done 1773\n",
            "Done 1777\n",
            "Done 1781\n",
            "Done 1783\n",
            "Done 1784\n",
            "Done 1785\n",
            "Done 1787\n",
            "Done 1788\n",
            "Done 1789\n",
            "Done 1795\n",
            "Done 1803\n",
            "Done 1805\n",
            "Done 1806\n",
            "Done 1809\n",
            "Done 1813\n",
            "Done 1814\n",
            "Done 1815\n",
            "Done 1816\n",
            "Done 1817\n",
            "Done 1819\n",
            "Done 1820\n",
            "Done 1821\n",
            "Done 1824\n",
            "Done 1825\n",
            "Done 1826\n",
            "Done 1830\n",
            "Done 1831\n",
            "Done 1832\n",
            "Done 1833\n",
            "Done 1834\n",
            "Done 1838\n",
            "Done 1839\n",
            "Done 1841\n",
            "Done 1844\n",
            "Done 1851\n",
            "Done 1854\n",
            "Done 1857\n",
            "Done 1859\n",
            "Done 1862\n",
            "Done 1865\n",
            "Done 1871\n",
            "Done 1873\n",
            "Done 1874\n",
            "Done 1875\n",
            "Done 1876\n",
            "Done 1877\n",
            "Done 1878\n",
            "Done 1884\n",
            "Done 1891\n",
            "Done 1895\n",
            "Done 1905\n",
            "Done 1906\n",
            "Done 1910\n",
            "Done 1914\n",
            "Done 1919\n",
            "Done 1921\n",
            "Done 1925\n",
            "Done 1930\n",
            "Done 1933\n",
            "Done 1935\n",
            "Done 1939\n",
            "Done 1945\n",
            "Done 1949\n",
            "Done 1950\n",
            "Done 1951\n",
            "Done 1952\n",
            "Done 1956\n",
            "Done 1957\n",
            "Done 1959\n",
            "Done 1961\n",
            "Done 1962\n",
            "Done 1963\n",
            "Done 1965\n",
            "Done 1970\n",
            "Done 1973\n",
            "Done 1975\n",
            "Done 1979\n",
            "Done 1983\n",
            "Done 1985\n",
            "Done 1989\n",
            "Done 1990\n",
            "Done 1991\n",
            "Done 1992\n",
            "Done 1994\n",
            "Done 1995\n",
            "Done 2002\n",
            "Done 2004\n",
            "Done 2005\n",
            "Done 2007\n",
            "Done 2008\n",
            "Done 2015\n",
            "Done 2016\n",
            "Done 2017\n",
            "Done 2021\n",
            "Done 2035\n",
            "Done 2036\n",
            "Done 2038\n",
            "Done 2039\n",
            "Done 2040\n",
            "Done 2043\n",
            "Done 2044\n",
            "Done 2050\n",
            "Done 2051\n",
            "Done 2052\n",
            "Done 2058\n",
            "Done 2060\n",
            "Done 2061\n",
            "Done 2066\n",
            "Done 2071\n",
            "Done 2077\n",
            "Done 2078\n",
            "Done 2082\n",
            "Done 2084\n",
            "Done 2092\n",
            "Done 2098\n",
            "Done 2100\n",
            "Done 2107\n",
            "Done 2108\n",
            "Done 2110\n",
            "Done 2113\n",
            "Done 2117\n",
            "Done 2118\n",
            "Done 2119\n",
            "Done 2123\n",
            "Done 2124\n",
            "Done 2125\n",
            "Done 2133\n",
            "Done 2134\n",
            "Done 2138\n",
            "Done 2141\n",
            "Done 2144\n",
            "Done 2150\n",
            "Done 2152\n",
            "Done 2156\n",
            "Done 2158\n",
            "Done 2160\n",
            "Done 2162\n",
            "Done 2163\n",
            "Done 2165\n",
            "Done 2166\n",
            "Done 2171\n",
            "Done 2173\n",
            "Done 2176\n",
            "Done 2177\n",
            "Done 2180\n",
            "Done 2182\n",
            "Done 2185\n",
            "Done 2186\n",
            "Done 2188\n",
            "Done 2189\n",
            "Done 2190\n",
            "Done 2193\n",
            "Done 2194\n",
            "Done 2196\n",
            "Done 2197\n",
            "Done 2198\n",
            "Done 2199\n",
            "Done 2201\n",
            "Done 2203\n",
            "Done 2204\n",
            "Done 2205\n",
            "Done 2207\n",
            "Done 2209\n",
            "Done 2213\n",
            "Done 2216\n",
            "Done 2222\n",
            "Done 2224\n",
            "Done 2231\n",
            "Done 2232\n",
            "Done 2234\n",
            "Done 2235\n",
            "Done 2242\n",
            "Done 2244\n",
            "Done 2245\n",
            "Done 2249\n",
            "Done 2256\n",
            "Done 2261\n",
            "Done 2265\n",
            "Done 2266\n",
            "Done 2271\n",
            "Done 2272\n",
            "Done 2275\n",
            "Done 2276\n",
            "Done 2280\n",
            "Done 2282\n",
            "Done 2285\n",
            "Done 2289\n",
            "Done 2296\n",
            "Done 2300\n",
            "Done 2305\n",
            "Done 2309\n",
            "Done 2312\n",
            "Done 2318\n",
            "Done 2321\n",
            "Done 2323\n",
            "Done 2324\n",
            "Done 2327\n",
            "Done 2328\n",
            "Done 2329\n",
            "Done 2330\n",
            "Done 2331\n",
            "Done 2337\n",
            "Done 2342\n",
            "Done 2345\n",
            "Done 2347\n",
            "Done 2353\n",
            "Done 2357\n",
            "Done 2358\n",
            "Done 2364\n",
            "Done 2366\n",
            "Done 2367\n",
            "Done 2368\n",
            "Done 2370\n",
            "Done 2380\n",
            "Done 2383\n",
            "Done 2386\n",
            "Done 2388\n",
            "Done 2389\n",
            "Done 2390\n",
            "Done 2393\n",
            "Done 2396\n",
            "Done 2398\n",
            "Done 2401\n",
            "Done 2404\n",
            "Done 2408\n",
            "Done 2410\n",
            "Done 2412\n",
            "Done 2413\n",
            "Done 2414\n",
            "Done 2416\n",
            "Done 2421\n",
            "Done 2424\n",
            "Done 2425\n",
            "Done 2428\n",
            "Done 2430\n",
            "Done 2431\n",
            "Done 2432\n",
            "Done 2436\n",
            "Done 2438\n",
            "Done 2439\n",
            "Done 2441\n",
            "Done 2442\n",
            "Done 2445\n",
            "Done 2448\n",
            "Done 2451\n",
            "Done 2455\n",
            "Done 2457\n",
            "Done 2459\n",
            "Done 2460\n",
            "Done 2461\n",
            "Done 2464\n",
            "Done 2467\n",
            "Done 2468\n",
            "Done 2474\n",
            "Done 2475\n",
            "Done 2478\n",
            "Done 2479\n",
            "Done 2482\n",
            "Done 2485\n",
            "Done 2487\n",
            "Done 2488\n",
            "Done 2490\n",
            "Done 2494\n",
            "Done 2495\n",
            "Done 2504\n",
            "Done 2510\n",
            "Done 2511\n",
            "Done 2513\n",
            "Done 2514\n",
            "Done 2517\n",
            "Done 2519\n",
            "Done 2520\n",
            "Done 2523\n",
            "Done 2527\n",
            "Done 2529\n",
            "Done 2530\n",
            "Done 2533\n",
            "Done 2538\n",
            "Done 2544\n",
            "Done 2546\n",
            "Done 2547\n",
            "Done 2548\n",
            "Done 2549\n",
            "Done 2550\n",
            "Done 2552\n",
            "Done 2557\n",
            "Done 2562\n",
            "Done 2566\n",
            "Done 2571\n",
            "Done 2576\n",
            "Done 2577\n",
            "Done 2586\n",
            "Done 2589\n",
            "Done 2591\n",
            "Done 2592\n",
            "Done 2593\n",
            "Done 2595\n",
            "Done 2599\n",
            "Done 2602\n",
            "Done 2603\n",
            "Done 2604\n",
            "Done 2605\n",
            "Done 2610\n",
            "Done 2611\n",
            "Done 2613\n",
            "Done 2618\n",
            "Done 2622\n",
            "Done 2624\n",
            "Done 2630\n",
            "Done 2632\n",
            "Done 2633\n",
            "Done 2636\n",
            "Done 2638\n",
            "Done 2644\n",
            "Done 2646\n",
            "Done 2648\n",
            "Done 2649\n",
            "Done 2650\n",
            "Done 2651\n",
            "Done 2652\n",
            "Done 2660\n",
            "Done 2661\n",
            "Done 2664\n",
            "Done 2665\n",
            "Done 2667\n",
            "Done 2669\n",
            "Done 2670\n",
            "Done 2674\n",
            "Done 2677\n",
            "Done 2680\n",
            "Done 2686\n",
            "Done 2687\n",
            "Done 2688\n",
            "Done 2692\n",
            "Done 2694\n",
            "Done 2697\n",
            "Done 2699\n",
            "Done 2701\n",
            "Done 2702\n",
            "Done 2705\n",
            "Done 2708\n",
            "Done 2711\n",
            "Done 2712\n",
            "Done 2721\n",
            "Done 2724\n",
            "Done 2729\n",
            "Done 2734\n",
            "Done 2735\n",
            "Done 2736\n",
            "Done 2739\n",
            "Done 2740\n",
            "Done 2741\n",
            "Done 2747\n",
            "Done 2748\n",
            "Done 2749\n",
            "Done 2754\n",
            "Done 2761\n",
            "Done 2765\n",
            "Done 2766\n",
            "Done 2769\n",
            "Done 2777\n",
            "Done 2780\n",
            "Done 2783\n",
            "Done 2784\n",
            "Done 2786\n",
            "Done 2787\n",
            "Done 2795\n",
            "Done 2797\n",
            "Done 2798\n",
            "Done 2800\n",
            "Done 2801\n",
            "Done 2802\n",
            "Done 2803\n",
            "Done 2804\n",
            "Done 2814\n",
            "Done 2821\n",
            "Done 2834\n",
            "Done 2836\n",
            "Done 2840\n",
            "Done 2843\n",
            "Done 2849\n",
            "Done 2850\n",
            "Done 2858\n",
            "Done 2859\n",
            "Done 2860\n",
            "Done 2861\n",
            "Done 2863\n",
            "Done 2864\n",
            "Done 2865\n",
            "Done 2867\n",
            "Done 2869\n",
            "Done 2872\n",
            "Done 2873\n",
            "Done 2877\n",
            "Done 2878\n",
            "Done 2880\n",
            "Done 2882\n",
            "Done 2885\n",
            "Done 2892\n",
            "Done 2893\n",
            "Done 2894\n",
            "Done 2895\n",
            "Done 2901\n",
            "Done 2902\n",
            "Done 2904\n",
            "Done 2909\n",
            "Done 2913\n",
            "Done 2914\n",
            "Done 2917\n",
            "Done 2921\n",
            "Done 2924\n",
            "Done 2927\n",
            "Done 2928\n",
            "Done 2933\n",
            "Done 2935\n",
            "Done 2939\n",
            "Done 2941\n",
            "Done 2944\n",
            "Done 2947\n",
            "Done 2953\n",
            "Done 2958\n",
            "Done 2959\n",
            "Done 2961\n",
            "Done 2964\n",
            "Done 2969\n",
            "Done 2974\n",
            "Done 2975\n",
            "Done 2976\n",
            "Done 2978\n",
            "Done 2980\n",
            "Done 2984\n",
            "Done 2985\n",
            "Done 2987\n",
            "Done 2988\n",
            "Done 2989\n",
            "Done 2992\n",
            "Done 2994\n",
            "Done 2996\n",
            "Done 2999\n",
            "Done 3000\n",
            "Done 3001\n",
            "Done 3002\n",
            "Done 3004\n",
            "Done 3005\n",
            "Done 3006\n",
            "Done 3009\n",
            "Done 3013\n",
            "Done 3014\n",
            "Done 3017\n",
            "Done 3020\n",
            "Done 3022\n",
            "Done 3023\n",
            "Done 3024\n",
            "Done 3026\n",
            "Done 3034\n",
            "Done 3036\n",
            "Done 3038\n",
            "Done 3041\n",
            "Done 3046\n",
            "Done 3047\n",
            "Done 3048\n",
            "Done 3049\n",
            "Done 3052\n",
            "Done 3053\n",
            "Done 3054\n",
            "Done 3055\n",
            "Done 3065\n",
            "Done 3070\n",
            "Done 3073\n",
            "Done 3077\n",
            "Done 3079\n",
            "Done 3081\n",
            "Done 3083\n",
            "Done 3084\n",
            "Done 3097\n",
            "Done 3099\n",
            "Done 3100\n",
            "Done 3104\n",
            "Done 3109\n",
            "Done 3114\n",
            "Done 3122\n",
            "Done 3126\n",
            "Done 3130\n",
            "Done 3134\n",
            "Done 3135\n",
            "Done 3136\n",
            "Done 3139\n",
            "Done 3143\n",
            "Done 3144\n",
            "Done 3148\n",
            "Done 3151\n",
            "Done 3156\n",
            "Done 3157\n",
            "Done 3163\n",
            "Done 3168\n",
            "Done 3171\n",
            "Done 3172\n",
            "Done 3174\n",
            "Done 3179\n",
            "Done 3180\n",
            "Done 3185\n",
            "Done 3186\n",
            "Done 3188\n",
            "Done 3191\n",
            "Done 3194\n",
            "Done 3196\n",
            "Done 3198\n",
            "Done 3201\n",
            "Done 3205\n",
            "Done 3206\n",
            "Done 3207\n",
            "Done 3211\n",
            "Done 3212\n",
            "Done 3214\n",
            "Done 3217\n",
            "Done 3218\n",
            "Done 3221\n",
            "Done 3222\n",
            "Done 3225\n",
            "Done 3226\n",
            "Done 3230\n",
            "Done 3231\n",
            "Done 3232\n",
            "Done 3233\n",
            "Done 3235\n",
            "Done 3236\n",
            "Done 3238\n",
            "Done 3239\n",
            "Done 3241\n",
            "Done 3244\n",
            "Done 3250\n",
            "Done 3251\n",
            "Done 3252\n",
            "Done 3254\n",
            "Done 3257\n",
            "Done 3258\n",
            "Done 3262\n",
            "Done 3263\n",
            "Done 3265\n",
            "Done 3266\n",
            "Done 3268\n",
            "Done 3269\n",
            "Done 3270\n",
            "Done 3271\n",
            "Done 3273\n",
            "Done 3274\n",
            "Done 3277\n",
            "Done 3278\n",
            "Done 3281\n",
            "Done 3283\n",
            "Done 3284\n",
            "Done 3285\n",
            "Done 3287\n",
            "Done 3296\n",
            "Done 3297\n",
            "Done 3299\n",
            "Done 3302\n",
            "Done 3304\n",
            "Done 3309\n",
            "Done 3311\n",
            "Done 3313\n",
            "Done 3317\n",
            "Done 3319\n",
            "Done 3322\n",
            "Done 3323\n",
            "Done 3326\n",
            "Done 3327\n",
            "Done 3328\n",
            "Done 3329\n",
            "Done 3331\n",
            "Done 3333\n",
            "Done 3335\n",
            "Done 3337\n",
            "Done 3341\n",
            "Done 3342\n",
            "Done 3346\n",
            "Done 3355\n",
            "Done 3357\n",
            "Done 3358\n",
            "Done 3360\n",
            "Done 3362\n",
            "Done 3363\n",
            "Done 3371\n",
            "Done 3372\n",
            "Done 3374\n",
            "Done 3375\n",
            "Done 3378\n",
            "Done 3384\n",
            "Done 3386\n",
            "Done 3388\n",
            "Done 3389\n",
            "Done 3392\n",
            "Done 3393\n",
            "Done 3394\n",
            "Done 3396\n",
            "Done 3397\n",
            "Done 3398\n",
            "Done 3402\n",
            "Done 3404\n",
            "Done 3405\n",
            "Done 3406\n",
            "Done 3407\n",
            "Done 3408\n",
            "Done 3411\n",
            "Done 3413\n",
            "Done 3415\n",
            "Done 3423\n",
            "Done 3424\n",
            "Done 3425\n",
            "Done 3430\n",
            "Done 3431\n",
            "Done 3433\n",
            "Done 3435\n",
            "Done 3438\n",
            "Done 3442\n",
            "Done 3448\n",
            "Done 3453\n",
            "Done 3454\n",
            "Done 3459\n",
            "Done 3460\n",
            "Done 3465\n",
            "Done 3467\n",
            "Done 3470\n",
            "Done 3477\n",
            "Done 3478\n",
            "Done 3483\n",
            "Done 3487\n",
            "Done 3490\n",
            "Done 3493\n",
            "Done 3494\n",
            "Done 3496\n",
            "Done 3500\n",
            "Done 3508\n",
            "Done 3510\n",
            "Done 3513\n",
            "Done 3514\n",
            "Done 3516\n",
            "Done 3517\n",
            "Done 3519\n",
            "Done 3520\n",
            "Done 3523\n",
            "Done 3525\n",
            "Done 3526\n",
            "Done 3528\n",
            "Done 3530\n",
            "Done 3538\n",
            "Done 3542\n",
            "Done 3543\n",
            "Done 3545\n",
            "Done 3552\n",
            "Done 3554\n",
            "Done 3555\n",
            "Done 3558\n",
            "Done 3565\n",
            "Done 3568\n",
            "Done 3573\n",
            "Done 3577\n",
            "Done 3578\n",
            "Done 3581\n",
            "Done 3585\n",
            "Done 3588\n",
            "Done 3591\n",
            "Done 3596\n",
            "Done 3598\n",
            "Done 3600\n",
            "Done 3603\n",
            "Done 3606\n",
            "Done 3613\n",
            "Done 3615\n",
            "Done 3616\n",
            "Done 3617\n",
            "Done 3619\n",
            "Done 3620\n",
            "Done 3625\n",
            "Done 3628\n",
            "Done 3630\n",
            "Done 3631\n",
            "Done 3632\n",
            "Done 3633\n",
            "Done 3636\n",
            "Done 3638\n",
            "Done 3647\n",
            "Done 3648\n",
            "Done 3656\n",
            "Done 3658\n",
            "Done 3660\n",
            "Done 3661\n",
            "Done 3662\n",
            "Done 3663\n",
            "Done 3664\n",
            "Done 3666\n",
            "Done 3667\n",
            "Done 3670\n",
            "Done 3671\n",
            "Done 3672\n",
            "Done 3675\n",
            "Done 3676\n",
            "Done 3678\n",
            "Done 3681\n",
            "Done 3687\n",
            "Done 3690\n",
            "Done 3693\n",
            "Done 3701\n",
            "Done 3708\n",
            "Done 3709\n",
            "Done 3713\n",
            "Done 3717\n",
            "Done 3721\n",
            "Done 3722\n",
            "Done 3726\n",
            "Done 3729\n",
            "Done 3733\n",
            "Done 3734\n",
            "Done 3738\n",
            "Done 3744\n",
            "Done 3754\n",
            "Done 3759\n",
            "Done 3760\n",
            "Done 3763\n",
            "Done 3764\n",
            "Done 3766\n",
            "Done 3767\n",
            "Done 3772\n",
            "Done 3777\n",
            "Done 3778\n",
            "Done 3779\n",
            "Done 3780\n",
            "Done 3783\n",
            "Done 3784\n",
            "Done 3786\n",
            "Done 3791\n",
            "Done 3792\n",
            "Done 3793\n",
            "Done 3794\n",
            "Done 3796\n",
            "Done 3797\n",
            "Done 3798\n",
            "Done 3802\n",
            "Done 3805\n",
            "Done 3813\n",
            "Done 3819\n",
            "Done 3822\n",
            "Done 3823\n",
            "Done 3830\n",
            "Done 3832\n",
            "Done 3834\n",
            "Done 3835\n",
            "Done 3836\n",
            "Done 3837\n",
            "Done 3839\n",
            "Done 3842\n",
            "Done 3844\n",
            "Done 3850\n",
            "Done 3852\n",
            "Done 3853\n",
            "Done 3859\n",
            "Done 3861\n",
            "Done 3868\n",
            "Done 3870\n",
            "Done 3876\n",
            "Done 3877\n",
            "Done 3883\n",
            "Done 3884\n",
            "Done 3885\n",
            "Done 3886\n",
            "Done 3890\n",
            "Done 3891\n",
            "Done 3894\n",
            "Done 3895\n",
            "Done 3897\n",
            "Done 3899\n",
            "Done 3900\n",
            "Done 3902\n",
            "Done 3909\n",
            "Done 3913\n",
            "Done 3914\n",
            "Done 3916\n",
            "Done 3921\n",
            "Done 3922\n",
            "Done 3925\n",
            "Done 3927\n",
            "Done 3928\n",
            "Done 3929\n",
            "Done 3936\n",
            "Done 3937\n",
            "Done 3939\n",
            "Done 3944\n",
            "Done 3947\n",
            "Done 3948\n",
            "Done 3952\n",
            "Done 3955\n",
            "Done 3956\n",
            "Done 3957\n",
            "Done 3959\n",
            "Done 3961\n",
            "Done 3962\n",
            "Done 3964\n",
            "Done 3965\n",
            "Done 3969\n",
            "Done 3970\n",
            "Done 3974\n",
            "Done 3976\n",
            "Done 3979\n",
            "Done 3984\n",
            "Done 3987\n",
            "Done 3989\n",
            "Done 3992\n",
            "Done 3993\n",
            "Done 3994\n",
            "Done 3996\n",
            "Done 3997\n",
            "Done 3999\n",
            "Done 4002\n",
            "Done 4004\n",
            "Done 4005\n",
            "Done 4008\n",
            "Done 4011\n",
            "Done 4014\n",
            "Done 4017\n",
            "Done 4023\n",
            "Done 4024\n",
            "Done 4025\n",
            "Done 4028\n",
            "Done 4032\n",
            "Done 4038\n",
            "Done 4043\n",
            "Done 4047\n",
            "Done 4050\n",
            "Done 4051\n",
            "Done 4052\n",
            "Done 4063\n",
            "Done 4067\n",
            "Done 4069\n",
            "Done 4072\n",
            "Done 4073\n",
            "Done 4076\n",
            "Done 4078\n",
            "Done 4080\n",
            "Done 4083\n",
            "Done 4086\n",
            "Done 4091\n",
            "Done 4093\n",
            "Done 4104\n",
            "Done 4111\n",
            "Done 4115\n",
            "Done 4118\n",
            "Done 4120\n",
            "Done 4121\n",
            "Done 4122\n",
            "Done 4127\n",
            "Done 4129\n",
            "Done 4131\n",
            "Done 4134\n",
            "Done 4135\n",
            "Done 4137\n",
            "Done 4138\n",
            "Done 4139\n",
            "Done 4141\n",
            "Done 4142\n",
            "Done 4143\n",
            "Done 4144\n",
            "Done 4145\n",
            "Done 4146\n",
            "Done 4148\n",
            "Done 4151\n",
            "Done 4154\n",
            "Done 4159\n",
            "Done 4160\n",
            "Done 4164\n",
            "Done 4165\n",
            "Done 4167\n",
            "Done 4168\n",
            "Done 4171\n",
            "Done 4177\n",
            "Done 4180\n",
            "Done 4181\n",
            "Done 4183\n",
            "Done 4184\n",
            "Done 4186\n",
            "Done 4189\n",
            "Done 4192\n",
            "Done 4210\n",
            "Done 4217\n",
            "Done 4220\n",
            "Done 4222\n",
            "Done 4226\n",
            "Done 4228\n",
            "Done 4229\n",
            "Done 4230\n",
            "Done 4232\n",
            "Done 4233\n",
            "Done 4234\n",
            "Done 4237\n",
            "Done 4239\n",
            "Done 4243\n",
            "Done 4246\n",
            "Done 4247\n",
            "Done 4255\n",
            "Done 4263\n",
            "Done 4264\n",
            "Done 4266\n",
            "Done 4276\n",
            "Done 4279\n",
            "Done 4286\n",
            "Done 4287\n",
            "Done 4290\n",
            "Done 4292\n",
            "Done 4295\n",
            "Done 4299\n",
            "Done 4300\n",
            "Done 4301\n",
            "Done 4303\n",
            "Done 4306\n",
            "Done 4310\n",
            "Done 4311\n",
            "Done 4313\n",
            "Done 4314\n",
            "Done 4317\n",
            "Done 4320\n",
            "Done 4322\n",
            "Done 4323\n",
            "Done 4324\n",
            "Done 4325\n",
            "Done 4329\n",
            "Done 4330\n",
            "Done 4331\n",
            "Done 4340\n",
            "Done 4343\n",
            "Done 4344\n",
            "Done 4346\n",
            "Done 4347\n",
            "Done 4349\n",
            "Done 4350\n",
            "Done 4353\n",
            "Done 4354\n",
            "Done 4355\n",
            "Done 4358\n",
            "Done 4359\n",
            "Done 4361\n",
            "Done 4366\n",
            "Done 4368\n",
            "Done 4369\n",
            "Done 4375\n",
            "Done 4379\n",
            "Done 4381\n",
            "Done 4382\n",
            "Done 4383\n",
            "Done 4385\n",
            "Done 4388\n",
            "Done 4390\n",
            "Done 4394\n",
            "Done 4399\n",
            "Done 4405\n",
            "Done 4406\n",
            "Done 4413\n",
            "Done 4415\n",
            "Done 4417\n",
            "Done 4422\n",
            "Done 4423\n",
            "Done 4424\n",
            "Done 4426\n",
            "Done 4429\n",
            "Done 4430\n",
            "Done 4431\n",
            "Done 4437\n",
            "Done 4442\n",
            "Done 4453\n",
            "Done 4454\n",
            "Done 4455\n",
            "Done 4459\n",
            "Done 4470\n",
            "Done 4473\n",
            "Done 4475\n",
            "Done 4476\n",
            "Done 4478\n",
            "Done 4486\n",
            "Done 4488\n",
            "Done 4489\n",
            "Done 4491\n",
            "Done 4494\n",
            "Done 4496\n",
            "Done 4497\n",
            "Done 4500\n",
            "Done 4503\n",
            "Done 4508\n",
            "Done 4509\n",
            "Done 4510\n",
            "Done 4511\n",
            "Done 4513\n",
            "Done 4514\n",
            "Done 4515\n",
            "Done 4516\n",
            "Done 4523\n",
            "Done 4524\n",
            "Done 4527\n",
            "Done 4529\n",
            "Done 4530\n",
            "Done 4531\n",
            "Done 4533\n",
            "Done 4537\n",
            "Done 4538\n",
            "Done 4541\n",
            "Done 4544\n",
            "Done 4545\n",
            "Done 4547\n",
            "Done 4555\n",
            "Done 4556\n",
            "Done 4558\n",
            "Done 4561\n",
            "Done 4565\n",
            "Done 4567\n",
            "Done 4568\n",
            "Done 4570\n",
            "Done 4572\n",
            "Done 4573\n",
            "Done 4577\n",
            "Done 4579\n",
            "Done 4581\n",
            "Done 4582\n",
            "Done 4589\n",
            "Done 4590\n",
            "Done 4591\n",
            "Done 4594\n",
            "Done 4596\n",
            "Done 4597\n",
            "Done 4602\n",
            "Done 4605\n",
            "Done 4607\n",
            "Done 4608\n",
            "Done 4611\n",
            "Done 4612\n",
            "Done 4613\n",
            "Done 4616\n",
            "Done 4619\n",
            "Done 4621\n",
            "Done 4622\n",
            "Done 4625\n",
            "Done 4629\n",
            "Done 4633\n",
            "Done 4634\n",
            "Done 4636\n",
            "Done 4639\n",
            "Done 4641\n",
            "Done 4642\n",
            "Done 4643\n",
            "Done 4645\n",
            "Done 4648\n",
            "Done 4650\n",
            "Done 4658\n",
            "Done 4661\n",
            "Done 4662\n",
            "Done 4663\n",
            "Done 4664\n",
            "Done 4665\n",
            "Done 4667\n",
            "Done 4668\n",
            "Done 4669\n",
            "Done 4670\n",
            "Done 4672\n",
            "Done 4673\n",
            "Done 4676\n",
            "Done 4677\n",
            "Done 4678\n",
            "Done 4679\n",
            "Done 4681\n",
            "Done 4683\n",
            "Done 4684\n",
            "Done 4687\n",
            "Done 4691\n",
            "Done 4694\n",
            "Done 4695\n",
            "Done 4697\n",
            "Done 4701\n",
            "Done 4705\n",
            "Done 4706\n",
            "Done 4709\n",
            "Done 4710\n",
            "Done 4714\n",
            "Done 4721\n",
            "Done 4724\n",
            "Done 4731\n",
            "Done 4732\n",
            "Done 4733\n",
            "Done 4735\n",
            "Done 4736\n",
            "Done 4738\n",
            "Done 4741\n",
            "Done 4744\n",
            "Done 4747\n",
            "Done 4750\n",
            "Done 4753\n",
            "Done 4761\n",
            "Done 4763\n",
            "Done 4766\n",
            "Done 4774\n",
            "Done 4776\n",
            "Done 4778\n",
            "Done 4780\n",
            "Done 4787\n",
            "Done 4788\n",
            "Done 4789\n",
            "Done 4791\n",
            "Done 4793\n",
            "Done 4794\n",
            "Done 4795\n",
            "Done 4797\n",
            "Done 4798\n",
            "Done 4800\n",
            "Done 4803\n",
            "Done 4811\n",
            "Done 4814\n",
            "Done 4817\n",
            "Done 4819\n",
            "Done 4821\n",
            "Done 4822\n",
            "Done 4823\n",
            "Done 4829\n",
            "Done 4831\n",
            "Done 4832\n",
            "Done 4833\n",
            "Done 4837\n",
            "Done 4843\n",
            "Done 4844\n",
            "Done 4851\n",
            "Done 4855\n",
            "Done 4857\n",
            "Done 4867\n",
            "Done 4868\n",
            "Done 4869\n",
            "Done 4870\n",
            "Done 4877\n",
            "Done 4878\n",
            "Done 4881\n",
            "Done 4889\n",
            "Done 4891\n",
            "Done 4895\n",
            "Done 4897\n",
            "Done 4898\n",
            "Done 4900\n",
            "Done 4903\n",
            "Done 4904\n",
            "Done 4907\n",
            "Done 4908\n",
            "Done 4912\n",
            "Done 4914\n",
            "Done 4921\n",
            "Done 4923\n",
            "Done 4925\n",
            "Done 4927\n",
            "Done 4928\n",
            "Done 4930\n",
            "Done 4933\n",
            "Done 4934\n",
            "Done 4936\n",
            "Done 4939\n",
            "Done 4940\n",
            "Done 4941\n",
            "Done 4943\n",
            "Done 4945\n",
            "Done 4946\n",
            "Done 4948\n",
            "Done 4949\n",
            "Done 4952\n",
            "Done 4954\n",
            "Done 4955\n",
            "Done 4958\n",
            "Done 4960\n",
            "Done 4961\n",
            "Done 4964\n",
            "Done 4968\n",
            "Done 4976\n",
            "Done 4984\n",
            "Done 4988\n",
            "Done 4994\n",
            "Done 4995\n",
            "Done 4998\n",
            "Done 5000\n",
            "Done 5007\n",
            "Done 5010\n",
            "Done 5017\n",
            "Done 5018\n",
            "Done 5024\n",
            "Done 5025\n",
            "Done 5026\n",
            "Done 5027\n",
            "Done 5029\n",
            "Done 5030\n",
            "Done 5034\n",
            "Done 5035\n",
            "Done 5038\n",
            "Done 5040\n",
            "Done 5041\n",
            "Done 5046\n",
            "Done 5048\n",
            "Done 5051\n",
            "Done 5052\n",
            "Done 5053\n",
            "Done 5056\n",
            "Done 5059\n",
            "Done 5066\n",
            "Done 5071\n",
            "Done 5077\n",
            "Done 5078\n",
            "Done 5079\n",
            "Done 5080\n",
            "Done 5084\n",
            "Done 5085\n",
            "Done 5089\n",
            "Done 5092\n",
            "Done 5098\n",
            "Done 5101\n",
            "Done 5103\n",
            "Done 5108\n",
            "Done 5110\n",
            "Done 5111\n",
            "Done 5112\n",
            "Done 5113\n",
            "Done 5119\n",
            "Done 5121\n",
            "Done 5124\n",
            "Done 5125\n",
            "Done 5126\n",
            "Done 5131\n",
            "Done 5132\n",
            "Done 5134\n",
            "Done 5135\n",
            "Done 5138\n",
            "Done 5139\n",
            "Done 5146\n",
            "Done 5150\n",
            "Done 5153\n",
            "Done 5155\n",
            "Done 5158\n",
            "Done 5159\n",
            "Done 5161\n",
            "Done 5165\n",
            "Done 5174\n",
            "Done 5178\n",
            "Done 5180\n",
            "Done 5185\n",
            "Done 5186\n",
            "Done 5193\n",
            "Done 5197\n",
            "Done 5203\n",
            "Done 5213\n",
            "Done 5216\n",
            "Done 5217\n",
            "Done 5220\n",
            "Done 5225\n",
            "Done 5229\n",
            "Done 5234\n",
            "Done 5236\n",
            "Done 5239\n",
            "Done 5240\n",
            "Done 5241\n",
            "Done 5242\n",
            "Done 5243\n",
            "Done 5244\n",
            "Done 5247\n",
            "Done 5248\n",
            "Done 5251\n",
            "Done 5254\n",
            "Done 5255\n",
            "Done 5258\n",
            "Done 5259\n",
            "Done 5262\n",
            "Done 5263\n",
            "Done 5266\n",
            "Done 5267\n",
            "Done 5268\n",
            "Done 5269\n"
          ]
        }
      ],
      "source": [
        "# for idx in range(len(lst_img)):\n",
        "#   try:\n",
        "#     img_dir = os.path.join(data_dir, str(lst_img[idx]) + '.jpg')\n",
        "#     img = cv2.imread(img_dir)\n",
        "#     img_resize = cv2.resize(img, (224,224), interpolation = cv2.INTER_AREA)\n",
        "#     if lst_label[idx] == 1:\n",
        "#       img_flip = cv2.resize(cv2.rotate(img_resize, cv2.cv2.ROTATE_90_CLOCKWISE), (224,224), interpolation = cv2.INTER_AREA)\n",
        "#       text_word = wordnet_aug.augment(lst_text[idx])[0]\n",
        "#       df.loc[len(df.index)] = ['flip_' +  str(lst_img[idx]), text_word, 1]\n",
        "#       cv2.imwrite(os.path.join('/content/dpl/data', 'flip_' +  str(lst_img[idx]) + '.jpg'), img_flip)\n",
        "\n",
        "#       img_gaus = cv2.resize(cv2.GaussianBlur(img_resize,(5,5),0), (224,224), interpolation = cv2.INTER_AREA)\n",
        "#       text_easy = easy_aug.augment(lst_text[idx])[0]\n",
        "#       df.loc[len(df.index)] = ['blur_' +  str(lst_img[idx]), text_easy, 1]\n",
        "#       cv2.imwrite(os.path.join('/content/dpl/data', 'blur_' +  str(lst_img[idx]) + '.jpg'), img_gaus)\n",
        "\n",
        "\n",
        "#     else:\n",
        "#       continue\n",
        "\n",
        "#     print(f'Done {idx}')\n",
        "#   except:\n",
        "#     error_id.append(idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJTHV7hm_ISe",
        "outputId": "a2795959-a8e6-4b3a-bc20-7501c62ee10a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9108"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(os.listdir('/content/dpl/data'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df.index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ela1PhUNC8Jo",
        "outputId": "b28e9f21-cfd6-4c3b-ca6b-d6e326fb4ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9108"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "8qrDBRQADdII",
        "outputId": "4df10ffa-b6cc-4b63-f66b-d02c38fd6dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             image_id                                               info  \\\n",
              "0          9517287663  130807-N-SK590-109 130807-N-SK590-109BANGA ISL...   \n",
              "1          4050082108  New Signs... How much on a flood plain archite...   \n",
              "2          6787040027  The Obelisk c.1818 This is a sandstone obelisk...   \n",
              "3          5633754452  Arena Boat Landing/Canoe Launch April 15, 2010...   \n",
              "4          5750126624                          clouds and the road None    \n",
              "...               ...                                                ...   \n",
              "9103  blur_8321062979  left meadows The river is on the Water of the ...   \n",
              "9104  flip_5932525414  None Until today Midland had 0.16\" of rain for...   \n",
              "9105  blur_5932525414  None Until today Midland had 0.16\" of rain lak...   \n",
              "9106  flip_6306491142  Thailand/Thai floods/ Chao Phraya river floodi...   \n",
              "9107  blur_6306491142  /Thai floods/ Chao Phraya river flooding large...   \n",
              "\n",
              "      label  \n",
              "0         0  \n",
              "1         0  \n",
              "2         0  \n",
              "3         1  \n",
              "4         0  \n",
              "...     ...  \n",
              "9103      1  \n",
              "9104      1  \n",
              "9105      1  \n",
              "9106      1  \n",
              "9107      1  \n",
              "\n",
              "[9108 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2797f6c6-98fd-4602-8e1f-9c686d393532\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>info</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9517287663</td>\n",
              "      <td>130807-N-SK590-109 130807-N-SK590-109BANGA ISL...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4050082108</td>\n",
              "      <td>New Signs... How much on a flood plain archite...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6787040027</td>\n",
              "      <td>The Obelisk c.1818 This is a sandstone obelisk...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5633754452</td>\n",
              "      <td>Arena Boat Landing/Canoe Launch April 15, 2010...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5750126624</td>\n",
              "      <td>clouds and the road None</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9103</th>\n",
              "      <td>blur_8321062979</td>\n",
              "      <td>left meadows The river is on the Water of the ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9104</th>\n",
              "      <td>flip_5932525414</td>\n",
              "      <td>None Until today Midland had 0.16\" of rain for...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9105</th>\n",
              "      <td>blur_5932525414</td>\n",
              "      <td>None Until today Midland had 0.16\" of rain lak...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9106</th>\n",
              "      <td>flip_6306491142</td>\n",
              "      <td>Thailand/Thai floods/ Chao Phraya river floodi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9107</th>\n",
              "      <td>blur_6306491142</td>\n",
              "      <td>/Thai floods/ Chao Phraya river flooding large...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9108 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2797f6c6-98fd-4602-8e1f-9c686d393532')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2797f6c6-98fd-4602-8e1f-9c686d393532 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2797f6c6-98fd-4602-8e1f-9c686d393532');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df.to_csv('/content/dpl/train.csv', index=False)"
      ],
      "metadata": {
        "id": "HiUBj53iDDgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split data into train, test and validation set"
      ],
      "metadata": {
        "id": "2RS-0cn_sX9C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qDALsCK3VGx"
      },
      "outputs": [],
      "source": [
        "perm = np.arange(len(text_arr))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "random.Random(126).shuffle(perm)"
      ],
      "metadata": {
        "id": "E9JQyluvOIa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yVZF0q73sIQ"
      },
      "outputs": [],
      "source": [
        "img_nparr = np.array(img_arr)[perm]\n",
        "text_nparr = np.array(text_arr)[perm]\n",
        "label_nparr = np.array(label_arr)[perm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEH82xuBAAGD"
      },
      "outputs": [],
      "source": [
        "val_size = int(img_nparr.shape[0]*0.1)\n",
        "test_size = int(img_nparr.shape[0]*0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhDkpBj7AFGy"
      },
      "outputs": [],
      "source": [
        "val_img = img_nparr[:val_size]\n",
        "val_text = text_nparr[:val_size]\n",
        "val_label = label_nparr[:val_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtjNYhdmAy8w"
      },
      "outputs": [],
      "source": [
        "test_img = img_nparr[val_size:val_size*2]\n",
        "test_text = text_nparr[val_size:val_size*2]\n",
        "test_label = label_nparr[val_size:val_size*2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnklEjW_EZUx"
      },
      "outputs": [],
      "source": [
        "train_img = img_nparr[val_size*2:]\n",
        "train_text = text_nparr[val_size*2:]\n",
        "train_label = label_nparr[val_size*2:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_nparr = []\n",
        "text_nparr = []\n",
        "label_nparr = []\n",
        "\n",
        "img_arr = []\n",
        "text_arr = []\n",
        "label_arr = []\n",
        "\n",
        "lst_img = []\n",
        "lst_text = []\n",
        "lst_label = []"
      ],
      "metadata": {
        "id": "9xnn5l1VOxX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLUc7SqzBFvn",
        "outputId": "43c9c371-dbb2-4f69-be05-768772d8b440"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5273"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(val_img) + len(test_img) + len(train_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW_ReSoSODxg"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOpr8VQfCEEw"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil \n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import models,layers,optimizers\n",
        "from keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.layers import Dropout, Flatten, Dense, BatchNormalization, Input, Concatenate\n",
        "\n",
        "#from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import EarlyStopping,  ModelCheckpoint, LearningRateScheduler\n",
        "\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhGCNy11Eu5v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization  # to create AdamW optimizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T776ah3m_inx"
      },
      "source": [
        "Image's Models  (VGG19, ResNet152, Inception)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ofHBO81xPxn",
        "outputId": "bc5698e7-08dd-4432-877c-47f6db44be18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "80150528/80134624 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#VGG-16 Model\n",
        "\n",
        "from operator import xor\n",
        "\n",
        "import keras \n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "\n",
        "image_input = Input(shape=(224, 224, 3), name='image')\n",
        "vgg19 = VGG19(include_top=False,\n",
        "              weights='imagenet',\n",
        "              input_shape=(224, 224, 3))(image_input)\n",
        "\n",
        "\n",
        "x = layers.Flatten()(vgg19) \n",
        "x = Dense(256, activation = \"relu\")(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(1, activation = \"sigmoid\")(x)\n",
        "# x = Dense(1024, activation = \"relu\")(x)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "\n",
        "# x = Dense(1024, activation = \"relu\")(x)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWW6EaZ2FakT",
        "outputId": "8f036259-83c9-40ca-f606-0e0761e77445"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234700800/234698864 [==============================] - 1s 0us/step\n",
            "234708992/234698864 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#Resnet \n",
        "import keras \n",
        "from tensorflow.keras.applications.resnet import ResNet152\n",
        "\n",
        "resnet_152 = ResNet152(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=(299,299,3)\n",
        ")\n",
        "\n",
        "for layer in resnet_152.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "\n",
        "image_input = Input(shape=(299, 299, 3), name='image')\n",
        "res = resnet_152(image_input)\n",
        "\n",
        "\n",
        "# incep = inception.get_layer(index = -1).output\n",
        "res = layers.Flatten() (res) \n",
        "res = Dense(256, activation = \"relu\") (res)\n",
        "\n",
        "res = Dense(4, activation='sigmoid') (res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0PUs9J5_kcR"
      },
      "outputs": [],
      "source": [
        "#Inception \n",
        "inception = tf.keras.applications.inception_v3.InceptionV3(\n",
        "      include_top=False,\n",
        "      weights=None,\n",
        "      input_shape=(224,224,3)\n",
        "  )\n",
        "\n",
        "for layer in inception.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "image_input = Input(shape=(224, 224, 3), name='image')\n",
        "incep = inception(image_input)\n",
        "\n",
        "\n",
        "# incep = inception.get_layer(index = -1).output\n",
        "incep = layers.Flatten()(incep) \n",
        "incep = Dense(256, activation = \"relu\")(incep)\n",
        "incep = Dense(1, activation='sigmoid')(incep)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WG-CSCRKGsXC"
      },
      "source": [
        "Bert pre-trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hd8YFRDKGz4r"
      },
      "outputs": [],
      "source": [
        "bert_model_name = 'small_bert/bert_en_uncased_L-12_H-768_A-12' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bi57lFvfGrBa"
      },
      "outputs": [],
      "source": [
        "map_name_to_handle = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/google/electra_small/2',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/google/electra_base/2',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
        "}\n",
        "\n",
        "map_model_to_preprocess = {\n",
        "    'bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_en_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'bert_multi_cased_L-12_H-768_A-12':\n",
        "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
        "    'albert_en_base':\n",
        "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
        "    'electra_small':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'electra_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_pubmed':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'experts_wiki_books':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "    'talking-heads_base':\n",
        "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCozbfojBfua"
      },
      "outputs": [],
      "source": [
        "#Bert model\n",
        "\n",
        "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
        "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCYxAX5PBlMQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a89446c-9fa0-4d46-93a5-fbe0ae805ca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using /tmp/tfhub_modules to cache modules.\n",
            "Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'.\n",
            "Downloaded https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3, Total size: 1.96MB\n",
            "Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'.\n"
          ]
        }
      ],
      "source": [
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyOtL-QrBaHs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da947a4e-77b3-4622-fa9d-bebd57e1939f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading TF-Hub Module 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1'.\n",
            "Downloaded https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1, Total size: 434.04MB\n",
            "Downloaded TF-Hub Module 'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1'.\n"
          ]
        }
      ],
      "source": [
        "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
        "encoder_inputs = preprocessing_layer(text_input)\n",
        "encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
        "outputs = encoder(encoder_inputs)\n",
        "net = outputs['pooled_output']\n",
        "net = tf.keras.layers.Dropout(0.1)(net)\n",
        "net = tf.keras.layers.Dense(1, activation='sigmoid')(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7_pPXAzGwk8"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGQAzEgFBoIh"
      },
      "outputs": [],
      "source": [
        "concate = Concatenate()([incep, net])\n",
        "out = Dense(1, activation='sigmoid') (concate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RUSUFtMB3QH"
      },
      "outputs": [],
      "source": [
        "mixed_model = Model([image_input, text_input], out)\n",
        "#mixed_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics =['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Po0_ebcwJ2w1"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D1pM0j6J2w2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f6e4cd9-b4be-49e2-857c-cd2b9b27f842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "using Adamw optimizer\n",
            "gradient_clip_norm=1.000000\n"
          ]
        }
      ],
      "source": [
        "epochs = 20\n",
        "steps_per_epoch = int(len(train_img) / 32)  + 1\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "num_warmup_steps = int(0.01*num_train_steps)\n",
        "\n",
        "init_lr = 3e-5\n",
        "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
        "                                          num_train_steps=num_train_steps,\n",
        "                                          num_warmup_steps=num_warmup_steps,\n",
        "                                          optimizer_type='adamw')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6JRGy7fJ2w2"
      },
      "outputs": [],
      "source": [
        "mixed_model.compile(optimizer=optimizer,\n",
        "                         loss=loss,\n",
        "                         metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je10JEKnpJf6",
        "outputId": "57a75797-7390-4b24-8837-6361f5342c48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " image (InputLayer)             [(None, 256, 256, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " inception_v3 (Functional)      (None, 6, 6, 2048)   21802784    ['image[0][0]']                  \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 73728)        0           ['inception_v3[0][0]']           \n",
            "                                                                                                  \n",
            " text (InputLayer)              [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 256)          18874624    ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " preprocessing (KerasLayer)     {'input_mask': (Non  0           ['text[0][0]']                   \n",
            "                                e, 128),                                                          \n",
            "                                 'input_type_ids':                                                \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_word_ids':                                                \n",
            "                                (None, 128)}                                                      \n",
            "                                                                                                  \n",
            " batch_normalization_189 (Batch  (None, 256)         1024        ['dense_2[0][0]']                \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " BERT_encoder (KerasLayer)      {'encoder_outputs':  109482241   ['preprocessing[0][0]',          \n",
            "                                 [(None, 128, 768),               'preprocessing[0][1]',          \n",
            "                                 (None, 128, 768),                'preprocessing[0][2]']          \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)],                                               \n",
            "                                 'pooled_output': (                                               \n",
            "                                None, 768),                                                       \n",
            "                                 'default': (None,                                                \n",
            "                                768),                                                             \n",
            "                                 'sequence_output':                                               \n",
            "                                 (None, 128, 768)}                                                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 256)          0           ['batch_normalization_189[0][0]']\n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 768)          0           ['BERT_encoder[0][13]']          \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            257         ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            769         ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 2)            0           ['dense_3[0][0]',                \n",
            "                                                                  'dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 1)            3           ['concatenate_5[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 150,161,702\n",
            "Trainable params: 128,358,405\n",
            "Non-trainable params: 21,803,297\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "mixed_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zKGM0XUFEQ2"
      },
      "outputs": [],
      "source": [
        "#mixed_model.load_weights('/content/drive/MyDrive/work/dpl/logs/epoch-13_loss-0.5155_val_loss-0.5311.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "id": "-HGMDDsiCDE_",
        "outputId": "3d339fc1-410a-4296-e581-4a374eafbb9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-4a49d2b1b031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                                         \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                                         min_delta=0.001)])\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 859, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py\", line 264, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 256, 256, 3), found shape=(None, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "history = mixed_model.fit([train_img, train_text],\n",
        "                          train_label,\n",
        "                          32,\n",
        "                          epochs=epochs,\n",
        "                          validation_data=([val_img, val_text], val_label),\n",
        "                          callbacks=[ModelCheckpoint('epoch-{epoch:02d}_loss-{loss:.4f}_val_loss-{val_loss:.4f}.h5',\n",
        "                                                          monitor='val_loss',\n",
        "                                                          verbose=1,\n",
        "                                                          save_best_only=True,\n",
        "                                                          save_weights_only=True,\n",
        "                                                          mode='auto',\n",
        "                                                          period=1),\n",
        "                                     EarlyStopping(monitor='val_loss',\n",
        "                                                        patience=7, \n",
        "                                                        verbose=1, \n",
        "                                                        min_delta=0.001)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZh7HqUdl0Io"
      },
      "outputs": [],
      "source": [
        "mixed_model.load_weights('/content/epoch-18_loss-0.6348_val_loss-0.6382.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTB1d3lmTfUC",
        "outputId": "2946f389-185a-4a30-c347-023d84bb75e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17/17 [==============================] - 7s 395ms/step - loss: 0.6505 - binary_accuracy: 0.8748\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.6504731774330139, 0.8747628331184387]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mixed_model.evaluate([test_img, test_text], test_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To submit csv file"
      ],
      "metadata": {
        "id": "DtNSciFtqlqM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3az843vC0DxB",
        "outputId": "d9a4ae47-6dbd-4151-995b-ea5df3db1286"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1320"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#stack df\n",
        "data_test_dir = '/content/dpl/testset_images/test'\n",
        "df_ts = pd.read_csv('/content/dpl/test.csv')\n",
        "df_ts['info'] = df['title'].astype(str) + \" \" + df['description'].astype(str) + \" \" + df['user_tags']\n",
        "len(df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8ca_wMF0DxB"
      },
      "outputs": [],
      "source": [
        "lst_img = df_ts['image_id'].to_list()\n",
        "lst_text = df_ts['info'].to_list()\n",
        "\n",
        "img_arr = []\n",
        "text_arr = []\n",
        "\n",
        "error_id = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFGXUp6vz2SC"
      },
      "outputs": [],
      "source": [
        "for idx in range(len(lst_img)):\n",
        "  img_dir = os.path.join(data_test_dir, str(lst_img[idx]) + '.jpg')\n",
        "  img = cv2.imread(img_dir)\n",
        "  img_resize = cv2.resize(img, (256,256), interpolation = cv2.INTER_AREA)\n",
        "  img_arr.append(img_resize)\n",
        "  text_arr.append(lst_text[idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctc3Zb4g05La"
      },
      "outputs": [],
      "source": [
        "devtest_img, devtest_text = np.array(img_arr), np.array(text_arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JCz2n-yvsJD"
      },
      "outputs": [],
      "source": [
        "predict_y = mixed_model.predict([devtest_img, devtest_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9aRYqJlDARR",
        "outputId": "a40a8162-cad9-443a-c0c1-f6934cf5f6cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           id  label\n",
            "0  3483809003      1\n",
            "1  3712805295      0\n",
            "2   379845620      0\n",
            "3  7343264988      1\n",
            "4  3843337492      1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/dpl/test.csv\")\n",
        "results = []\n",
        "for predict in predict_y:\n",
        "  if predict > 0.5:\n",
        "    res = 1\n",
        "  else:\n",
        "    res = 0\n",
        "  results.append(res)\n",
        "# print(results)\n",
        "#df = df.drop('Text', 1)\n",
        "df.insert(1, \"label\", results, True)\n",
        "# df = df[df[\"Label\"] == 1]\n",
        "df = df.rename(columns={'image_id' : 'id'})\n",
        "df = df.drop(['title', 'description', 'user_tags'], 1)\n",
        "print(df.head())\n",
        "\n",
        "df.to_csv ('text_submit.csv', index = False, header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test TF-IDF"
      ],
      "metadata": {
        "id": "PNvLHASerPsR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-nj4aictFrQ"
      },
      "outputs": [],
      "source": [
        "#SKlearn TF-IDF\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=128, stop_words='english')\n",
        "\n",
        "text_vec = vectorizer.fit_transform(train_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pagu3bgwtfwR"
      },
      "outputs": [],
      "source": [
        "vtext_train = text_vec.toarray()\n",
        "vtext_val = vectorizer.fit_transform(val_text).toarray()\n",
        "y_train = train_label.reshape((-1,1))\n",
        "y_val = val_label.reshape((-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otN2CzO2tpBh"
      },
      "outputs": [],
      "source": [
        "input_text = Input(shape=(128), name='text_vec')\n",
        "\n",
        "textmodel = Dense(32, activation='relu') (input_text)\n",
        "textmodel = Dense(1, activation='sigmoid') (textmodel)\n",
        "\n",
        "model = Model(input_text, textmodel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q7a3NXauSpB"
      },
      "outputs": [],
      "source": [
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "metrics = tf.metrics.BinaryAccuracy()\n",
        "\n",
        "model.compile(optimizer='RMSprop', loss=loss, metrics=metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRbgVNRMuoiZ",
        "outputId": "cda21c2d-4cbf-4e0c-df0a-f48148fd48af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "132/132 [==============================] - 1s 7ms/step - loss: 0.6016 - binary_accuracy: 0.6710 - val_loss: 0.6290 - val_binary_accuracy: 0.6300\n",
            "Epoch 2/200\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.4635 - binary_accuracy: 0.7798 - val_loss: 0.6081 - val_binary_accuracy: 0.6603\n",
            "Epoch 3/200\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.3873 - binary_accuracy: 0.8220 - val_loss: 0.6182 - val_binary_accuracy: 0.6717\n",
            "Epoch 4/200\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.3606 - binary_accuracy: 0.8343 - val_loss: 0.6591 - val_binary_accuracy: 0.6698\n",
            "Epoch 5/200\n",
            "132/132 [==============================] - 1s 5ms/step - loss: 0.3478 - binary_accuracy: 0.8360 - val_loss: 0.6538 - val_binary_accuracy: 0.6793\n",
            "Epoch 6/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.3409 - binary_accuracy: 0.8433 - val_loss: 0.6782 - val_binary_accuracy: 0.6831\n",
            "Epoch 7/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.3347 - binary_accuracy: 0.8466 - val_loss: 0.7114 - val_binary_accuracy: 0.6774\n",
            "Epoch 8/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.3298 - binary_accuracy: 0.8478 - val_loss: 0.7256 - val_binary_accuracy: 0.6793\n",
            "Epoch 9/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.3253 - binary_accuracy: 0.8488 - val_loss: 0.7386 - val_binary_accuracy: 0.6774\n",
            "Epoch 10/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.3215 - binary_accuracy: 0.8526 - val_loss: 0.7770 - val_binary_accuracy: 0.6774\n",
            "Epoch 11/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.3182 - binary_accuracy: 0.8526 - val_loss: 0.7823 - val_binary_accuracy: 0.6850\n",
            "Epoch 12/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.3152 - binary_accuracy: 0.8557 - val_loss: 0.7968 - val_binary_accuracy: 0.6907\n",
            "Epoch 13/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.3118 - binary_accuracy: 0.8573 - val_loss: 0.8219 - val_binary_accuracy: 0.6907\n",
            "Epoch 14/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.3089 - binary_accuracy: 0.8587 - val_loss: 0.8299 - val_binary_accuracy: 0.6926\n",
            "Epoch 15/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.3062 - binary_accuracy: 0.8604 - val_loss: 0.8468 - val_binary_accuracy: 0.6831\n",
            "Epoch 16/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.3032 - binary_accuracy: 0.8618 - val_loss: 0.8753 - val_binary_accuracy: 0.6812\n",
            "Epoch 17/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.3008 - binary_accuracy: 0.8625 - val_loss: 0.8950 - val_binary_accuracy: 0.6850\n",
            "Epoch 18/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2987 - binary_accuracy: 0.8613 - val_loss: 0.8999 - val_binary_accuracy: 0.6888\n",
            "Epoch 19/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2964 - binary_accuracy: 0.8642 - val_loss: 0.9321 - val_binary_accuracy: 0.6926\n",
            "Epoch 20/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2941 - binary_accuracy: 0.8668 - val_loss: 0.9563 - val_binary_accuracy: 0.6869\n",
            "Epoch 21/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2926 - binary_accuracy: 0.8644 - val_loss: 0.9567 - val_binary_accuracy: 0.6850\n",
            "Epoch 22/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2899 - binary_accuracy: 0.8663 - val_loss: 0.9882 - val_binary_accuracy: 0.6907\n",
            "Epoch 23/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2881 - binary_accuracy: 0.8670 - val_loss: 0.9571 - val_binary_accuracy: 0.6907\n",
            "Epoch 24/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2867 - binary_accuracy: 0.8687 - val_loss: 0.9938 - val_binary_accuracy: 0.6888\n",
            "Epoch 25/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2847 - binary_accuracy: 0.8692 - val_loss: 1.0085 - val_binary_accuracy: 0.6850\n",
            "Epoch 26/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2835 - binary_accuracy: 0.8701 - val_loss: 1.0257 - val_binary_accuracy: 0.6888\n",
            "Epoch 27/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2817 - binary_accuracy: 0.8722 - val_loss: 1.0477 - val_binary_accuracy: 0.6869\n",
            "Epoch 28/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2801 - binary_accuracy: 0.8720 - val_loss: 1.0804 - val_binary_accuracy: 0.6869\n",
            "Epoch 29/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2787 - binary_accuracy: 0.8730 - val_loss: 1.0532 - val_binary_accuracy: 0.6831\n",
            "Epoch 30/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2773 - binary_accuracy: 0.8741 - val_loss: 1.0590 - val_binary_accuracy: 0.6945\n",
            "Epoch 31/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2761 - binary_accuracy: 0.8737 - val_loss: 1.1039 - val_binary_accuracy: 0.6850\n",
            "Epoch 32/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2748 - binary_accuracy: 0.8760 - val_loss: 1.0835 - val_binary_accuracy: 0.6907\n",
            "Epoch 33/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2734 - binary_accuracy: 0.8753 - val_loss: 1.0982 - val_binary_accuracy: 0.6869\n",
            "Epoch 34/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2720 - binary_accuracy: 0.8777 - val_loss: 1.1297 - val_binary_accuracy: 0.6812\n",
            "Epoch 35/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2708 - binary_accuracy: 0.8775 - val_loss: 1.1558 - val_binary_accuracy: 0.6869\n",
            "Epoch 36/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2696 - binary_accuracy: 0.8784 - val_loss: 1.1639 - val_binary_accuracy: 0.6850\n",
            "Epoch 37/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2686 - binary_accuracy: 0.8794 - val_loss: 1.1416 - val_binary_accuracy: 0.6888\n",
            "Epoch 38/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2675 - binary_accuracy: 0.8794 - val_loss: 1.1395 - val_binary_accuracy: 0.6907\n",
            "Epoch 39/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2666 - binary_accuracy: 0.8794 - val_loss: 1.1751 - val_binary_accuracy: 0.6869\n",
            "Epoch 40/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2654 - binary_accuracy: 0.8817 - val_loss: 1.1828 - val_binary_accuracy: 0.6888\n",
            "Epoch 41/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2639 - binary_accuracy: 0.8815 - val_loss: 1.1648 - val_binary_accuracy: 0.6926\n",
            "Epoch 42/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2632 - binary_accuracy: 0.8824 - val_loss: 1.2080 - val_binary_accuracy: 0.6869\n",
            "Epoch 43/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2620 - binary_accuracy: 0.8822 - val_loss: 1.2410 - val_binary_accuracy: 0.6869\n",
            "Epoch 44/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2607 - binary_accuracy: 0.8839 - val_loss: 1.2431 - val_binary_accuracy: 0.6888\n",
            "Epoch 45/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2601 - binary_accuracy: 0.8858 - val_loss: 1.2261 - val_binary_accuracy: 0.6907\n",
            "Epoch 46/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2589 - binary_accuracy: 0.8834 - val_loss: 1.2115 - val_binary_accuracy: 0.6888\n",
            "Epoch 47/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2581 - binary_accuracy: 0.8843 - val_loss: 1.2441 - val_binary_accuracy: 0.6888\n",
            "Epoch 48/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2570 - binary_accuracy: 0.8850 - val_loss: 1.2801 - val_binary_accuracy: 0.6831\n",
            "Epoch 49/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2554 - binary_accuracy: 0.8869 - val_loss: 1.2540 - val_binary_accuracy: 0.6850\n",
            "Epoch 50/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2551 - binary_accuracy: 0.8850 - val_loss: 1.2726 - val_binary_accuracy: 0.6812\n",
            "Epoch 51/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2539 - binary_accuracy: 0.8879 - val_loss: 1.2843 - val_binary_accuracy: 0.6793\n",
            "Epoch 52/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2528 - binary_accuracy: 0.8877 - val_loss: 1.3016 - val_binary_accuracy: 0.6812\n",
            "Epoch 53/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2520 - binary_accuracy: 0.8888 - val_loss: 1.3162 - val_binary_accuracy: 0.6812\n",
            "Epoch 54/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2508 - binary_accuracy: 0.8888 - val_loss: 1.3568 - val_binary_accuracy: 0.6774\n",
            "Epoch 55/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2500 - binary_accuracy: 0.8886 - val_loss: 1.3187 - val_binary_accuracy: 0.6831\n",
            "Epoch 56/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2494 - binary_accuracy: 0.8910 - val_loss: 1.3274 - val_binary_accuracy: 0.6812\n",
            "Epoch 57/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2483 - binary_accuracy: 0.8907 - val_loss: 1.3554 - val_binary_accuracy: 0.6812\n",
            "Epoch 58/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2472 - binary_accuracy: 0.8943 - val_loss: 1.3418 - val_binary_accuracy: 0.6812\n",
            "Epoch 59/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2466 - binary_accuracy: 0.8929 - val_loss: 1.3592 - val_binary_accuracy: 0.6812\n",
            "Epoch 60/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2454 - binary_accuracy: 0.8936 - val_loss: 1.3790 - val_binary_accuracy: 0.6755\n",
            "Epoch 61/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2442 - binary_accuracy: 0.8933 - val_loss: 1.3567 - val_binary_accuracy: 0.6812\n",
            "Epoch 62/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2438 - binary_accuracy: 0.8938 - val_loss: 1.4072 - val_binary_accuracy: 0.6774\n",
            "Epoch 63/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2430 - binary_accuracy: 0.8948 - val_loss: 1.4078 - val_binary_accuracy: 0.6774\n",
            "Epoch 64/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2419 - binary_accuracy: 0.8938 - val_loss: 1.4292 - val_binary_accuracy: 0.6774\n",
            "Epoch 65/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2413 - binary_accuracy: 0.8957 - val_loss: 1.4174 - val_binary_accuracy: 0.6793\n",
            "Epoch 66/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2402 - binary_accuracy: 0.8974 - val_loss: 1.4235 - val_binary_accuracy: 0.6793\n",
            "Epoch 67/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2391 - binary_accuracy: 0.8969 - val_loss: 1.3908 - val_binary_accuracy: 0.6812\n",
            "Epoch 68/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2387 - binary_accuracy: 0.8969 - val_loss: 1.4485 - val_binary_accuracy: 0.6793\n",
            "Epoch 69/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2376 - binary_accuracy: 0.8967 - val_loss: 1.4656 - val_binary_accuracy: 0.6755\n",
            "Epoch 70/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2366 - binary_accuracy: 0.8986 - val_loss: 1.4642 - val_binary_accuracy: 0.6774\n",
            "Epoch 71/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2360 - binary_accuracy: 0.8993 - val_loss: 1.4476 - val_binary_accuracy: 0.6774\n",
            "Epoch 72/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2349 - binary_accuracy: 0.9007 - val_loss: 1.4216 - val_binary_accuracy: 0.6774\n",
            "Epoch 73/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2341 - binary_accuracy: 0.8993 - val_loss: 1.4569 - val_binary_accuracy: 0.6774\n",
            "Epoch 74/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2333 - binary_accuracy: 0.9007 - val_loss: 1.4635 - val_binary_accuracy: 0.6774\n",
            "Epoch 75/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2323 - binary_accuracy: 0.9019 - val_loss: 1.5203 - val_binary_accuracy: 0.6755\n",
            "Epoch 76/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2315 - binary_accuracy: 0.9033 - val_loss: 1.5018 - val_binary_accuracy: 0.6736\n",
            "Epoch 77/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2304 - binary_accuracy: 0.9031 - val_loss: 1.5170 - val_binary_accuracy: 0.6736\n",
            "Epoch 78/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2299 - binary_accuracy: 0.9033 - val_loss: 1.5067 - val_binary_accuracy: 0.6736\n",
            "Epoch 79/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2290 - binary_accuracy: 0.9021 - val_loss: 1.5051 - val_binary_accuracy: 0.6736\n",
            "Epoch 80/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2279 - binary_accuracy: 0.9026 - val_loss: 1.5782 - val_binary_accuracy: 0.6698\n",
            "Epoch 81/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2272 - binary_accuracy: 0.9033 - val_loss: 1.5816 - val_binary_accuracy: 0.6736\n",
            "Epoch 82/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2263 - binary_accuracy: 0.9047 - val_loss: 1.5548 - val_binary_accuracy: 0.6736\n",
            "Epoch 83/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2254 - binary_accuracy: 0.9054 - val_loss: 1.5725 - val_binary_accuracy: 0.6736\n",
            "Epoch 84/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2248 - binary_accuracy: 0.9054 - val_loss: 1.5706 - val_binary_accuracy: 0.6717\n",
            "Epoch 85/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2240 - binary_accuracy: 0.9059 - val_loss: 1.5873 - val_binary_accuracy: 0.6717\n",
            "Epoch 86/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2231 - binary_accuracy: 0.9071 - val_loss: 1.5935 - val_binary_accuracy: 0.6717\n",
            "Epoch 87/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2224 - binary_accuracy: 0.9073 - val_loss: 1.6156 - val_binary_accuracy: 0.6698\n",
            "Epoch 88/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2216 - binary_accuracy: 0.9076 - val_loss: 1.6202 - val_binary_accuracy: 0.6698\n",
            "Epoch 89/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2210 - binary_accuracy: 0.9085 - val_loss: 1.6100 - val_binary_accuracy: 0.6698\n",
            "Epoch 90/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2199 - binary_accuracy: 0.9097 - val_loss: 1.6252 - val_binary_accuracy: 0.6717\n",
            "Epoch 91/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2191 - binary_accuracy: 0.9087 - val_loss: 1.6488 - val_binary_accuracy: 0.6698\n",
            "Epoch 92/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2186 - binary_accuracy: 0.9097 - val_loss: 1.6373 - val_binary_accuracy: 0.6717\n",
            "Epoch 93/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2175 - binary_accuracy: 0.9104 - val_loss: 1.6278 - val_binary_accuracy: 0.6755\n",
            "Epoch 94/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2170 - binary_accuracy: 0.9095 - val_loss: 1.6632 - val_binary_accuracy: 0.6717\n",
            "Epoch 95/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2161 - binary_accuracy: 0.9104 - val_loss: 1.6857 - val_binary_accuracy: 0.6698\n",
            "Epoch 96/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2153 - binary_accuracy: 0.9104 - val_loss: 1.6930 - val_binary_accuracy: 0.6698\n",
            "Epoch 97/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2144 - binary_accuracy: 0.9106 - val_loss: 1.7023 - val_binary_accuracy: 0.6717\n",
            "Epoch 98/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2136 - binary_accuracy: 0.9135 - val_loss: 1.7277 - val_binary_accuracy: 0.6717\n",
            "Epoch 99/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2129 - binary_accuracy: 0.9111 - val_loss: 1.7148 - val_binary_accuracy: 0.6717\n",
            "Epoch 100/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2123 - binary_accuracy: 0.9149 - val_loss: 1.6968 - val_binary_accuracy: 0.6736\n",
            "Epoch 101/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2114 - binary_accuracy: 0.9135 - val_loss: 1.7148 - val_binary_accuracy: 0.6736\n",
            "Epoch 102/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2105 - binary_accuracy: 0.9132 - val_loss: 1.7594 - val_binary_accuracy: 0.6679\n",
            "Epoch 103/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2100 - binary_accuracy: 0.9161 - val_loss: 1.7259 - val_binary_accuracy: 0.6736\n",
            "Epoch 104/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2091 - binary_accuracy: 0.9161 - val_loss: 1.7403 - val_binary_accuracy: 0.6736\n",
            "Epoch 105/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2085 - binary_accuracy: 0.9156 - val_loss: 1.7275 - val_binary_accuracy: 0.6736\n",
            "Epoch 106/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2079 - binary_accuracy: 0.9161 - val_loss: 1.7692 - val_binary_accuracy: 0.6736\n",
            "Epoch 107/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2071 - binary_accuracy: 0.9163 - val_loss: 1.7788 - val_binary_accuracy: 0.6717\n",
            "Epoch 108/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2064 - binary_accuracy: 0.9166 - val_loss: 1.7601 - val_binary_accuracy: 0.6736\n",
            "Epoch 109/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2056 - binary_accuracy: 0.9175 - val_loss: 1.7735 - val_binary_accuracy: 0.6736\n",
            "Epoch 110/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2047 - binary_accuracy: 0.9170 - val_loss: 1.7855 - val_binary_accuracy: 0.6679\n",
            "Epoch 111/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2040 - binary_accuracy: 0.9178 - val_loss: 1.8286 - val_binary_accuracy: 0.6679\n",
            "Epoch 112/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2033 - binary_accuracy: 0.9178 - val_loss: 1.8260 - val_binary_accuracy: 0.6660\n",
            "Epoch 113/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.2027 - binary_accuracy: 0.9166 - val_loss: 1.7935 - val_binary_accuracy: 0.6698\n",
            "Epoch 114/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2021 - binary_accuracy: 0.9173 - val_loss: 1.8249 - val_binary_accuracy: 0.6660\n",
            "Epoch 115/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.2014 - binary_accuracy: 0.9180 - val_loss: 1.8506 - val_binary_accuracy: 0.6679\n",
            "Epoch 116/200\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.2006 - binary_accuracy: 0.9194 - val_loss: 1.8443 - val_binary_accuracy: 0.6679\n",
            "Epoch 117/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1997 - binary_accuracy: 0.9192 - val_loss: 1.8728 - val_binary_accuracy: 0.6660\n",
            "Epoch 118/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1991 - binary_accuracy: 0.9196 - val_loss: 1.8117 - val_binary_accuracy: 0.6736\n",
            "Epoch 119/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1987 - binary_accuracy: 0.9201 - val_loss: 1.8793 - val_binary_accuracy: 0.6641\n",
            "Epoch 120/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1981 - binary_accuracy: 0.9204 - val_loss: 1.8638 - val_binary_accuracy: 0.6660\n",
            "Epoch 121/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1974 - binary_accuracy: 0.9199 - val_loss: 1.8702 - val_binary_accuracy: 0.6641\n",
            "Epoch 122/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1966 - binary_accuracy: 0.9211 - val_loss: 1.8634 - val_binary_accuracy: 0.6660\n",
            "Epoch 123/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1963 - binary_accuracy: 0.9213 - val_loss: 1.8906 - val_binary_accuracy: 0.6622\n",
            "Epoch 124/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1954 - binary_accuracy: 0.9206 - val_loss: 1.8854 - val_binary_accuracy: 0.6641\n",
            "Epoch 125/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1945 - binary_accuracy: 0.9204 - val_loss: 1.8485 - val_binary_accuracy: 0.6679\n",
            "Epoch 126/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1939 - binary_accuracy: 0.9208 - val_loss: 1.8634 - val_binary_accuracy: 0.6679\n",
            "Epoch 127/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1934 - binary_accuracy: 0.9218 - val_loss: 1.8783 - val_binary_accuracy: 0.6679\n",
            "Epoch 128/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1928 - binary_accuracy: 0.9220 - val_loss: 1.9144 - val_binary_accuracy: 0.6622\n",
            "Epoch 129/200\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1921 - binary_accuracy: 0.9232 - val_loss: 1.9041 - val_binary_accuracy: 0.6641\n",
            "Epoch 130/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1913 - binary_accuracy: 0.9223 - val_loss: 1.9496 - val_binary_accuracy: 0.6622\n",
            "Epoch 131/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1907 - binary_accuracy: 0.9237 - val_loss: 1.9197 - val_binary_accuracy: 0.6660\n",
            "Epoch 132/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1902 - binary_accuracy: 0.9234 - val_loss: 1.9571 - val_binary_accuracy: 0.6641\n",
            "Epoch 133/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1893 - binary_accuracy: 0.9244 - val_loss: 1.9255 - val_binary_accuracy: 0.6641\n",
            "Epoch 134/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1888 - binary_accuracy: 0.9244 - val_loss: 1.9148 - val_binary_accuracy: 0.6660\n",
            "Epoch 135/200\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1883 - binary_accuracy: 0.9246 - val_loss: 1.9660 - val_binary_accuracy: 0.6641\n",
            "Epoch 136/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1877 - binary_accuracy: 0.9230 - val_loss: 1.9739 - val_binary_accuracy: 0.6641\n",
            "Epoch 137/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1869 - binary_accuracy: 0.9244 - val_loss: 1.9686 - val_binary_accuracy: 0.6641\n",
            "Epoch 138/200\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1862 - binary_accuracy: 0.9232 - val_loss: 1.9722 - val_binary_accuracy: 0.6641\n",
            "Epoch 139/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1859 - binary_accuracy: 0.9249 - val_loss: 2.0187 - val_binary_accuracy: 0.6698\n",
            "Epoch 140/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1851 - binary_accuracy: 0.9253 - val_loss: 1.9818 - val_binary_accuracy: 0.6641\n",
            "Epoch 141/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1844 - binary_accuracy: 0.9249 - val_loss: 1.9751 - val_binary_accuracy: 0.6641\n",
            "Epoch 142/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1838 - binary_accuracy: 0.9260 - val_loss: 2.0380 - val_binary_accuracy: 0.6679\n",
            "Epoch 143/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1830 - binary_accuracy: 0.9265 - val_loss: 2.0630 - val_binary_accuracy: 0.6679\n",
            "Epoch 144/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1822 - binary_accuracy: 0.9256 - val_loss: 2.0363 - val_binary_accuracy: 0.6679\n",
            "Epoch 145/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1821 - binary_accuracy: 0.9258 - val_loss: 2.0622 - val_binary_accuracy: 0.6660\n",
            "Epoch 146/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1815 - binary_accuracy: 0.9270 - val_loss: 2.0636 - val_binary_accuracy: 0.6660\n",
            "Epoch 147/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1807 - binary_accuracy: 0.9287 - val_loss: 2.0713 - val_binary_accuracy: 0.6679\n",
            "Epoch 148/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1801 - binary_accuracy: 0.9275 - val_loss: 2.0508 - val_binary_accuracy: 0.6641\n",
            "Epoch 149/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1795 - binary_accuracy: 0.9291 - val_loss: 2.0833 - val_binary_accuracy: 0.6679\n",
            "Epoch 150/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1789 - binary_accuracy: 0.9296 - val_loss: 2.0687 - val_binary_accuracy: 0.6641\n",
            "Epoch 151/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1787 - binary_accuracy: 0.9287 - val_loss: 2.1047 - val_binary_accuracy: 0.6660\n",
            "Epoch 152/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1775 - binary_accuracy: 0.9272 - val_loss: 2.0962 - val_binary_accuracy: 0.6641\n",
            "Epoch 153/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1770 - binary_accuracy: 0.9291 - val_loss: 2.1108 - val_binary_accuracy: 0.6660\n",
            "Epoch 154/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1764 - binary_accuracy: 0.9289 - val_loss: 2.0748 - val_binary_accuracy: 0.6641\n",
            "Epoch 155/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1762 - binary_accuracy: 0.9294 - val_loss: 2.1162 - val_binary_accuracy: 0.6641\n",
            "Epoch 156/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1751 - binary_accuracy: 0.9296 - val_loss: 2.1422 - val_binary_accuracy: 0.6679\n",
            "Epoch 157/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1748 - binary_accuracy: 0.9298 - val_loss: 2.1489 - val_binary_accuracy: 0.6679\n",
            "Epoch 158/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1740 - binary_accuracy: 0.9298 - val_loss: 2.1080 - val_binary_accuracy: 0.6641\n",
            "Epoch 159/200\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1736 - binary_accuracy: 0.9294 - val_loss: 2.1174 - val_binary_accuracy: 0.6641\n",
            "Epoch 160/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1730 - binary_accuracy: 0.9296 - val_loss: 2.1887 - val_binary_accuracy: 0.6717\n",
            "Epoch 161/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1725 - binary_accuracy: 0.9310 - val_loss: 2.2062 - val_binary_accuracy: 0.6717\n",
            "Epoch 162/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1718 - binary_accuracy: 0.9291 - val_loss: 2.1529 - val_binary_accuracy: 0.6679\n",
            "Epoch 163/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1711 - binary_accuracy: 0.9306 - val_loss: 2.1967 - val_binary_accuracy: 0.6717\n",
            "Epoch 164/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1710 - binary_accuracy: 0.9315 - val_loss: 2.1667 - val_binary_accuracy: 0.6660\n",
            "Epoch 165/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1705 - binary_accuracy: 0.9306 - val_loss: 2.1935 - val_binary_accuracy: 0.6698\n",
            "Epoch 166/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1696 - binary_accuracy: 0.9298 - val_loss: 2.1746 - val_binary_accuracy: 0.6660\n",
            "Epoch 167/200\n",
            "132/132 [==============================] - 1s 4ms/step - loss: 0.1690 - binary_accuracy: 0.9308 - val_loss: 2.1879 - val_binary_accuracy: 0.6660\n",
            "Epoch 168/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1687 - binary_accuracy: 0.9317 - val_loss: 2.2233 - val_binary_accuracy: 0.6698\n",
            "Epoch 169/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1679 - binary_accuracy: 0.9324 - val_loss: 2.2383 - val_binary_accuracy: 0.6698\n",
            "Epoch 170/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1672 - binary_accuracy: 0.9322 - val_loss: 2.2138 - val_binary_accuracy: 0.6660\n",
            "Epoch 171/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1669 - binary_accuracy: 0.9322 - val_loss: 2.2356 - val_binary_accuracy: 0.6660\n",
            "Epoch 172/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1661 - binary_accuracy: 0.9332 - val_loss: 2.2614 - val_binary_accuracy: 0.6698\n",
            "Epoch 173/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1659 - binary_accuracy: 0.9324 - val_loss: 2.2839 - val_binary_accuracy: 0.6698\n",
            "Epoch 174/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1652 - binary_accuracy: 0.9327 - val_loss: 2.2882 - val_binary_accuracy: 0.6698\n",
            "Epoch 175/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1645 - binary_accuracy: 0.9320 - val_loss: 2.2753 - val_binary_accuracy: 0.6679\n",
            "Epoch 176/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1645 - binary_accuracy: 0.9324 - val_loss: 2.3139 - val_binary_accuracy: 0.6698\n",
            "Epoch 177/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1635 - binary_accuracy: 0.9324 - val_loss: 2.2991 - val_binary_accuracy: 0.6679\n",
            "Epoch 178/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1636 - binary_accuracy: 0.9315 - val_loss: 2.3561 - val_binary_accuracy: 0.6698\n",
            "Epoch 179/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1624 - binary_accuracy: 0.9334 - val_loss: 2.3053 - val_binary_accuracy: 0.6679\n",
            "Epoch 180/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1622 - binary_accuracy: 0.9327 - val_loss: 2.3131 - val_binary_accuracy: 0.6698\n",
            "Epoch 181/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1616 - binary_accuracy: 0.9334 - val_loss: 2.3308 - val_binary_accuracy: 0.6698\n",
            "Epoch 182/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1612 - binary_accuracy: 0.9322 - val_loss: 2.3293 - val_binary_accuracy: 0.6717\n",
            "Epoch 183/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1606 - binary_accuracy: 0.9341 - val_loss: 2.3850 - val_binary_accuracy: 0.6698\n",
            "Epoch 184/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1601 - binary_accuracy: 0.9341 - val_loss: 2.3901 - val_binary_accuracy: 0.6679\n",
            "Epoch 185/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1594 - binary_accuracy: 0.9348 - val_loss: 2.3961 - val_binary_accuracy: 0.6679\n",
            "Epoch 186/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1592 - binary_accuracy: 0.9351 - val_loss: 2.3465 - val_binary_accuracy: 0.6698\n",
            "Epoch 187/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1584 - binary_accuracy: 0.9353 - val_loss: 2.3735 - val_binary_accuracy: 0.6717\n",
            "Epoch 188/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1582 - binary_accuracy: 0.9346 - val_loss: 2.4178 - val_binary_accuracy: 0.6660\n",
            "Epoch 189/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1575 - binary_accuracy: 0.9353 - val_loss: 2.4691 - val_binary_accuracy: 0.6603\n",
            "Epoch 190/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1568 - binary_accuracy: 0.9341 - val_loss: 2.3744 - val_binary_accuracy: 0.6698\n",
            "Epoch 191/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1564 - binary_accuracy: 0.9355 - val_loss: 2.4319 - val_binary_accuracy: 0.6622\n",
            "Epoch 192/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1556 - binary_accuracy: 0.9353 - val_loss: 2.4052 - val_binary_accuracy: 0.6698\n",
            "Epoch 193/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1557 - binary_accuracy: 0.9360 - val_loss: 2.4702 - val_binary_accuracy: 0.6622\n",
            "Epoch 194/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1550 - binary_accuracy: 0.9358 - val_loss: 2.4732 - val_binary_accuracy: 0.6641\n",
            "Epoch 195/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1539 - binary_accuracy: 0.9353 - val_loss: 2.3990 - val_binary_accuracy: 0.6717\n",
            "Epoch 196/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1542 - binary_accuracy: 0.9370 - val_loss: 2.4877 - val_binary_accuracy: 0.6622\n",
            "Epoch 197/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1533 - binary_accuracy: 0.9360 - val_loss: 2.5342 - val_binary_accuracy: 0.6603\n",
            "Epoch 198/200\n",
            "132/132 [==============================] - 0s 3ms/step - loss: 0.1530 - binary_accuracy: 0.9374 - val_loss: 2.4889 - val_binary_accuracy: 0.6641\n",
            "Epoch 199/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1526 - binary_accuracy: 0.9379 - val_loss: 2.5010 - val_binary_accuracy: 0.6641\n",
            "Epoch 200/200\n",
            "132/132 [==============================] - 0s 4ms/step - loss: 0.1523 - binary_accuracy: 0.9381 - val_loss: 2.5223 - val_binary_accuracy: 0.6641\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f3cb0da4610>"
            ]
          },
          "execution_count": 195,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(vtext_train, \n",
        "          y_train,\n",
        "          32,\n",
        "          epochs=200,\n",
        "          validation_data=(vtext_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o29_lC10xXwI"
      },
      "outputs": [],
      "source": [
        "vtext_test = vectorizer.fit_transform(test_text).toarray()\n",
        "y_test = test_label.reshape((-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifxNGmmDxgFE",
        "outputId": "c04917dd-1a7b-4704-e681-30f9994f0425"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17/17 [==============================] - 0s 2ms/step - loss: 1.0549 - binary_accuracy: 0.7362\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[1.0548866987228394, 0.7362428903579712]"
            ]
          },
          "execution_count": 197,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.evaluate(vtext_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Im2gf9YYYOW"
      },
      "source": [
        "# SUBMIT CSV KAGGLE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQi41N5uDnNn"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/dpl/test.csv\")\n",
        "\n",
        "df['info'] = df['title'].astype(str) + \" \" + df['description'].astype(str) + \" \" + df['user_tags']\n",
        "\n",
        "submit_set = df['info'].values.astype('U')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULZHajo_Fc6l"
      },
      "outputs": [],
      "source": [
        "submits = vectorizer.fit_transform(submit_set).toarray()\n",
        "result_model = model.predict(submits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ioujb8WqIAPQ",
        "outputId": "52c0052f-3728-4b3c-ca81-a8a6f3538609"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "           id  label\n",
            "0  3483809003      0\n",
            "1  3712805295      1\n",
            "2   379845620      0\n",
            "3  7343264988      0\n",
            "4  3843337492      0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
            "  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/dpl/test.csv\")\n",
        "results = []\n",
        "for predict in result_model:\n",
        "  if predict > 0.5:\n",
        "    res = 1\n",
        "  else:\n",
        "    res = 0\n",
        "  results.append(res)\n",
        "# print(results)\n",
        "#df = df.drop('Text', 1)\n",
        "df.insert(1, \"label\", results, True)\n",
        "# df = df[df[\"Label\"] == 1]\n",
        "df = df.rename(columns={'image_id' : 'id'})\n",
        "df = df.drop(['title', 'description', 'user_tags'], 1)\n",
        "print(df.head())\n",
        "\n",
        "df.to_csv ('text_submit.csv', index = False, header=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DPL302m_Kaggle_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}